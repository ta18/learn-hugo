---
title: "DÃ©tection d'objets avec tensorflow2"
menu:
  main:
    name: "DÃ©tection d'objets avec tf2"
    weight: 3
    parent: "vision"
---

Dans cette section nous allons utiliser l'API __Tensorflow Object Detection__ (_a.k.a_ TOD) qui propose :
* une collection de rÃ©seaux prÃ©-entraÃ®nÃ©s, spÃ©cialement conÃ§us pour pour la dÃ©tection d'objets dans des images (__Object Detection__),
* un mÃ©canisme de _transfert learning_ pour continuer l'entraÃ®nement des rÃ©seaux prÃ©-entraÃ®nÃ©s avec ses propres images labellisÃ©es, 
pour obtenir la dÃ©tection des objets qui nous intÃ©ressent.

Contrairement Ã  la stratÃ©gie de __Classification__ prÃ©sentÃ©e dans la section [Classification tf2](https://learn.e.ros4.pro/fr/vision/classification_tf2/), 
la __DÃ©tection d'objets__ permet de trouver directement les boÃ®tes englobantes des objets "face avec un 1" et "face avec un 2" : 
cette approche Ã©vite de faire appel au traitement d'image classique pour extraire les faces des cubes dans un premier temps, puis de classifier les images des faces des cubes dans un deuxiÃ¨me temps. 

Le traitement d'image utilisÃ© pour la classification est basÃ© sur une approche traditionnelle de manipulation des pixels de l'image (seuillage, extraction de contour, segmentation...).
Il est assez fragile : sensible Ã  la luminositÃ©, Ã  la prÃ©sence ou non d'un fond noir...

Un avantage attendu de l'approcje __Object Detection__ est de fournir directement les boÃ®tes englobantes des faces des cubes, sans passer par l'Ã©tape de traitement d'image.

## PrÃ©requis

* BAC+2 et +
* Bonne comprÃ©hension de Python et numpy
* Une premiÃ¨re expÃ©rience des rÃ©seaux de neurones est souhaitable.

L'entraÃ®nement des rÃ©seaux de neurones avec le module `tensorflow` se fera de prÃ©fÃ©rence dans un environnement virtuel Python (EVP) qui permet de travailler dans un environnement Python  sÃ©parÃ© de celui existant pour le travail sous ROS.

ğŸ’» Utilise la [FAQ Python : environnement virtuel](https://learn.e.ros4.pro/fr/faq/venv/)  pour crÃ©er un EVP :
* nommÃ© `tf2`, 
* avec une version de Python Ã©gale Ã  `3.8`.
## 1. Documentation

1. Documentation gÃ©nÃ©rale sur numpy :
	* [Numpy cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)
	* [NumPy quickstart](https://numpy.org/devdocs/user/quickstart.html)

2. Documentation sur l'_API TOD_ pour `tensorflow2` :
	* Le tutoriel officiel complet : [TensorFlow 2 Object Detection API tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html)
	* Le dÃ©pÃ´t git : [models/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)<br><br>
Le tutoriel peut Ãªtre consultÃ© pour aller chercher des dÃ©tails qui ne sont pas dÃ©veloppÃ©s dans l'activitÃ© proposÃ©e, mais il est prÃ©ferrable de suivre 
les indications du document prÃ©sent pour installer et utiliser rapidement une version rÃ©cente de tensorflow2. 

3. Lectures complÃ©mentaires :
	* [1] [Zero to Hero: Guide to Object Detection using Deep Learning: Faster R-CNN,YOLO,SSD](https://cv-tricks.com/object-detection/faster-r-cnn-yolo-ssd/)
	* [2] [mAP (mean Average Precision) for Object Detection](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173)
	* [3] [Understanding SSD MultiBox â€” Real-Time Object Detection In Deep Learning](https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab)

## 2. Installer l'API TOD

L'installation de l'API TOD se dÃ©roule en 5 Ã©tapes :
1. CrÃ©er et initialiser ton espace de travail
2. Cloner le dÃ©pÃ´t `tensorflow/models`
3. Installer les outils `protobuf`
4. Installer l'API COCO
5. Installer le package `object_detection` 

Dans tout le document le _prompt_ du terminal sera notÃ© `(tf2) jlc@pikatchou $` : le prÃ©fixe `(tf2)` est lÃ  pour bien rappeler que le travail Python pour l'API TOD se fait 
dans l'__Environnement Virtuel Python tf2__ que tu auras crÃ©Ã© au prÃ©alable (cf les PrÃ©requis).


### 2.1 CrÃ©er et initialiser ton espace de travail

La premiÃ¨re Ã©tape consiste Ã  crÃ©er le rÃ©pertoire de travail `tod_tf2`, dans lequel seront crÃ©Ã©s tous les fichiers, et de te positionner dans ce rÃ©pertoire qui sera __le dossier racine du projet__.
:
```bash
(tf2) jlc@pikatchou $ cd <quelque_part>   # choisis le rÃ©pertoire oÃ¹ crÃ©er `tod_tf2`, par exemple "cd ~/catkins_ws"
(tf2) jlc@pikatchou $ mkdir tod_tf2
(tf2) jlc@pikatchou $ cd tod_tf2/
```
ğŸ“¥ Ensuite, tu clones le dÃ©pÃ´t github `cjlux/tod_tf2_tools.git` et tu copies les fichiers `*.py` et `*.ipynb` du dossier `tod_tf2_tools` dans le dossier `tod_tf2` : 
```bash
# From tod_tf2/
(tf2) jlc@pikatchou $ git clone https://github.com/cjlux/tod_tf2_tools.git
(tf2) jlc@pikatchou $ cp tod_tf2_tools/*.py .
(tf2) jlc@pikatchou $ cp tod_tf2_tools/*.ipynb .
```

### 2.2 Cloner le dÃ©pÃ´t `tensorflow/models`

ğŸ“¥ Dans le dossier de travail `tod_tf2` clone le dÃ©pÃ´t github `tensorflow/models`Â (~ 635 Mo) :
```bash
# From tod_tf2/
(tf2) jlc@pikatchou $ git clone https://github.com/tensorflow/models.git
```

Tu obtiens un dossier `models`. Lâ€™API TOD est dans le dossier `models/research/object_detection` :
```bash	
(tf2) jlc@pikatchou $ tree -d -L 2 .
.
â””â”€â”€ models
    â”œâ”€â”€ community
    â”œâ”€â”€ official
    â”œâ”€â”€ orbit
    â””â”€â”€ research
```	

ğŸ“¥ ComplÃ¨te ton installation avec quelques paquets Python utiles pour le travail avec l'API TOD :

```bash
(tf2) jlc@pikatchou $ conda install cython contextlib2 pillow lxml
(tf2) jlc@pikatchou $ pip install labelimg rospkg
```
Mets Ã  jour la variable dâ€™environnement `PYTHONPATH` en ajoutant Ã  la fin du fichier `~/.bashrc` les deux lignesÂ :
```bash
export TOD_ROOT="<chemin absolu du dossier tod_tf2>"
export PYTHONPATH=$TOD_ROOT/models:$TOD_ROOT/models/research:$PYTHONPATH
```
remplace `"<chemin absolu du dossier tod_tf2>"` par le chemin absolu du dossier `tod_tf2` sur ta machine.

* Lance un nouveau terminal pour activer le nouvel environnement shellÂ : tout ce qui suit sera fait dans ce nouveau terminal.
* âš ï¸ n'oublie pas d'activer l'EVP `tf2` dans ce nouveau terminal :
```bash
jlc@pikatchou $ conda activate tf2
(tf2) jlc@pikatchou $
 ```

### 2.3 Installer les outils `protobuf`

Lâ€™API native TOD utilise des fichiers `*.proto`Â pour la configuration des modÃ¨les et le stockage des paramÃ¨tres dâ€™entraÃ®nement. 
Ces fichiers doivent Ãªtre traduits en fichiers `*.py` afin que lâ€™API Python puisse fonctionner correctement : 

* Installe d'abord le paquet debian `protobuf-compile` qui donne accÃ¨s Ã  la commande `protoc`Â :
```bash
(tf2) jlc@pikatchou $ sudo apt install protobuf-compiler
```
* Tu peux ensuite te positionner dans le dossier `tod_tf2/models/research` et taperÂ :
```bash
# From tod_tf2/models/research/
(tf2) jlc@pikatchou $ protoc object_detection/protos/*.proto  --python_out=.
```
Cette commande travaille de faÃ§on muette.

### 2.4 Installer l'API COCO

COCO est une banque de donnÃ©es destinÃ©e Ã  alimenter les algorithmes de dÃ©tection dâ€™objets, de segmentationâ€¦ <br>
Voir [cocodataset.org](https://cocodataset.org) pour les tutoriels, publicationsâ€¦ 

ğŸ“¥ Pour installer lâ€™API Python de COCO, clone le site `cocoapi.git` (~ 15 Mo) dans le dossier `/tmp`, tape la commande `make` dans le dossier `cocoapi/PythonAPI`, puis recopie le dossier `pycococtools` dans ton dossier `.../models/research/`Â :
```bash
(tf2) jlc@pikatchou $ cd /tmp
(tf2) jlc@pikatchou $ git clone  https://github.com/cocodataset/cocoapi.git
(tf2) jlc@pikatchou $ cd cocoapi/PythonAPI/
(tf2) jlc@pikatchou $ make
(tf2) jlc@pikatchou $ cp -r pycocotools/ <chemin absolu du dossier tod_tf2>/models/research/
```
remplace `"<chemin absolu du dossier tod_tf2>"` par le chemin absolu du dossier `tod_tf2` sur ta machine (par exemple `~/catkins_ws/tod_tf2`).

### 2.5 Installer le package `object_detection` 

Pour finir l'installation, place-toi dans le dossier  `models/research/` et tape les commandes :
```bash
# From tod_tf2/models/research/
(tf2) jlc@pikatchou $ cp object_detection/packages/tf2/setup.py .
(tf2) jlc@pikatchou $ python setup.py build
(tf2) jlc@pikatchou $ pip install .
```

### 2.6 Tester l'installation de l'API TOD

Pour tester ton installation de lâ€™API TOD, place-toi dans le dossier `models/research/` et tape la commandeÂ :
```bash	
# From within tod_tf2/models/research/
(tf2) jlc@pikatchou $ python object_detection/builders/model_builder_tf2_test.py
```
Le programme dÃ©roule toute une sÃ©rie de tests et doit se terminer par un OKÂ sans fiare apparaÃ®tre d'erreur :

	...
	[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size
	[ RUN      ] ModelBuilderTF2Test.test_session
	[  SKIPPED ] ModelBuilderTF2Test.test_session
	[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s
	I0505 18:19:38.639148 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor
	[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s
	I0505 18:19:38.640017 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture
	[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s
	I0505 18:19:38.641987 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor
	----------------------------------------------------------------------
	Ran 21 tests in 53.105s

	OK (skipped=1)

Pour finir, tu peux vÃ©rifier lâ€™installation en utilisant le notebook IPython `object_detection_tutorial.ipynb` prÃ©sent dans le dossier `tod_tf2`.<br>
(note : c'est une copie du notebook `tod_tf2/models/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb` dans laquelle on a enlevÃ© les cellules d'installation de l'API_TOD et quelques autres cellules qui peuvent gÃ©nÃ©rer des erreurs...).

* âš ï¸ Avant d'exÃ©cuter les cellules du notebook, il faut corriger une erreur dans le fichier `.../tod_tf2/models/research/object_detection/utils/ops.py`, ligne 825 :
remplace `tf.uint8` par `tf.uint8.as_numpy_dtype`

* Dans le dossier `tod_tf2` lance la commande `jupyter notebook` et charge le notebook `object_detection_tutorial.ipynb`.
* ExÃ©cute les cellules une Ã  une, tu ne dois pas avoir dâ€™erreur :

	* La partie "__Detection__" (qui dure de quelques secondes Ã   plusieurs minutes suivant ton CPUâ€¦) utilise le rÃ©seau prÃ©-entraÃ®nÃ© `ssd_mobilenet_v1_coco_2017_11_17` pour dÃ©tecter des objets dans les   imagesÂ de test :	
![notebook_test_TOD_image1et2.png](img/notebook_test_TOD_image1et2.png)

	* La partie "__Instance Segmentation__" est plus gourmande en ressources (jusqu'Ã  8 Go de RAM) et dure de quelques dizaines de secondes Ã  plusieurs dizaines de minutes suivant ton CPU ; elle utilise le rÃ©seau prÃ©-entraÃ®nÃ© `mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28` pour dÃ©tecter les objets et leurs masques, par exemple :
![notebook_test_TOD_image-mask1.png](img/notebook_test_TOD_image-mask1.png)

La suite du travail se dÃ©compose ainsi :
* ComplÃ©ter l'arborescence de travail
* TÃ©lÃ©charger le rÃ©seau prÃ©-entraÃ®nÃ©
* CrÃ©er la banque d'images labellisÃ©es pour l'entraÃ®nement supervisÃ© du rÃ©seau choisi
* EntraÃ®ner le rÃ©seau avec la banque d'images labellisÃ©es.
* Ã‰valuer les infÃ©rences du rÃ©seau avec les images de test
* IntÃ©grer l'exploitation du rÃ©seau dans l'environnement ROS.

## 3. ComplÃ©ter l'arborescence de travail

L'arborescence gÃ©nÃ©rique proposÃ©e est la suivante :

	tod_tf2
	â”œâ”€â”€ images
	â”‚   â””â”€â”€<project>
	â”‚       â”œâ”€â”€ test
	â”‚       â”‚   â””â”€â”€ *.jpg, *.png ... *.xml
	â”‚       â”œâ”€â”€ train
	â”‚       â”‚   â””â”€â”€ *.jpg, *.png ... *.xml
	â”‚       â””â”€â”€ *.csv
	â”œâ”€â”€ pre_trained
	â”‚	â””â”€â”€ <pre_trained-network>
	â”œâ”€â”€ training
	â”‚   â””â”€â”€<project>
	â”‚       â”œâ”€â”€ <pre_trained-network>
	â”‚       â”œâ”€â”€ train.record
	â”‚       â”œâ”€â”€ test.record
	â”‚       â””â”€â”€ label_map.txt
	â””â”€â”€ models
	    â””â”€â”€ research
	        â””â”€â”€ object_detection
	
* Tout ce qui est spÃ©cifique au projet est placÃ© dans un rÃ©pertoire `<project>` Ã  diffÃ©rents niveaux.

* Le dossier `images/<project>` contient pour chaque projet :
	* les dossiers `test` et `train` qui contiennent chacun :
		* les images PNG, JPG... Ã  analyser,
		* les fichiers d'annotation XML crÃ©Ã©s avec le logiciel `labelImg` : ils donnent, pour chacun des objets d'une image, les coordonnÃ©es de la boÃ®te englobant l'objet et le label de l'objet.
	* les fichiers d'annotation CSV (contenu des fichiers XML converti au format CSV), qui seront Ã  leur tour convertis au format _tensorflow record_.
* Le dossier `pre_trained/` contient un sous-dossier pour chacun des rÃ©seaux prÃ©-entrainÃ©s utilisÃ©.
* le dossier `training/<project>` contient pour chaque projet :
	* un dossier pour rÃ©seau prÃ©-entrainÃ© utilisÃ© : c'est dans ce dossier que sont stockÃ©s les fichiers des poids du rÃ©seau entraÃ®nÃ©,
	* les fichiers `train.reccord`  et `test.reccord` : contiennent les donnÃ©es labelisÃ©es d'entraÃ®nement et de test converties du format CSV au format _tensorflow record_,
	* le fichier `label_map.txt` : liste les labels correspondants aux objets Ã  dÃ©tecter.
	
Pour la dÃ©tection des faces des cubes dans les images de la camÃ©ra du robot, le dossier `<project>` sera nommÃ© `faces_cubes`, ce qui donne l'arborescence de travail :

	tod_tf2
	â”œâ”€â”€ images
	â”‚   â””â”€â”€ faces_cubes
	â”‚       â”œâ”€â”€ test
	â”‚       â”‚   â””â”€â”€ *.jpg, *.png ... *.xml
	â”‚       â”œâ”€â”€ train
	â”‚       â”‚   â””â”€â”€ *.jpg, *.png ... *.xml
	â”‚       â””â”€â”€ *.csv
	â”œâ”€â”€ pre_trained
	â”‚	â””â”€â”€ <pre_trained-network>
	â”œâ”€â”€ training
	â”‚   â””â”€â”€ faces_cubes
	â”‚       â”œâ”€â”€â”€ <pre_trained-network>
	â”‚       â”œâ”€â”€ train.record
	â”‚       â”œâ”€â”€ test.record
	â”‚       â””â”€â”€ label_map.txt
	â””â”€â”€ models
	    â””â”€â”€ research
	        â””â”€â”€ object_detection

Quelques commandes shell suffisent pour crÃ©er les premiers niveaux de cette arborescence :

```bash	
# From within tod_tf2
(tf2) jlc@pikatchou $ mkdir -p images/faces_cubes/test
(tf2) jlc@pikatchou $ mkdir -p images/faces_cubes/train
(tf2) jlc@pikatchou $ mkdir pre_trained
(tf2) jlc@pikatchou $ mkdir -p training/faces_cubes
```
VÃ©rifions :
```bash	
# From within tod_tf2
(tf2) jlc@pikatchou $ tree -d . -I models
.
â”œâ”€â”€ images
â”‚Â Â  â””â”€â”€ faces_cubes
â”‚Â Â      â”œâ”€â”€ test
â”‚Â Â      â””â”€â”€ train
â”œâ”€â”€ pre_trained
â”œâ”€â”€ tod_tf2_tools
â””â”€â”€ training
    â””â”€â”€ faces_cubes
```
	 
## 4. TÃ©lÃ©charger le rÃ©seau prÃ©-entraÃ®nÃ©

Plusieurs familles de rÃ©seaux dÃ©diÃ©s Ã  la dÃ©tection dâ€™objets sont proposÃ©s sur le site  [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md), parmi lesquellesÂ :

* Les rÃ©seaux __R-CNN__ (_Region-based Convolutional Neural Network_) : basÃ©s sur le concept de __recherche ciblÃ©e__ (_selective search_). 
![R-CNN](img/R-CNN.png)(source: https://arxiv.org/pdf/1311.2524.pdf)<br>
Au lieu dâ€™appliquer la sous-fenÃªtre d'analyse Ã  toutes les positions possibles dans lâ€™image, lâ€™algorithme de recherche ciblÃ©e gÃ©nÃ¨re 2000 propositions de rÃ©gions dâ€™intÃ©rÃªts oÃ¹ il est le plus probable de trouver des objets Ã  dÃ©tecter. Cet algorithme se base sur des Ã©lÃ©ments tels que la texture, lâ€™intensitÃ© et la couleur des objets quâ€™il a appris Ã  dÃ©tecter pour proposer des rÃ©gions dâ€™intÃ©rÃªt. Une fois les 2000 rÃ©gions choisies, la derniÃ¨re partie du rÃ©seau calcule la probabilitÃ© que lâ€™objet dans la rÃ©gion appartienne Ã  chaque classe. Les versions __Fast R-CNN__ et __Faster R-CNN__ rendent lâ€™entraÃ®nement plus efficace et plus rapide.

* Les rÃ©seaux __SSD__ (_Single Shot Detector_) : font partie des dÃ©tecteurs considÃ©rant la dÃ©tection dâ€™objets comme un problÃ¨me de rÃ©gression. L'algorithme __SSD__ utilise dâ€™abord un rÃ©seau de neurones convolutif pour produire une carte des points clÃ©s dans lâ€™image puis, comme __Faster R-CNN__, utilise des cadres de diffÃ©rentes tailles pour traiter les Ã©chelles et les ratios dâ€™aspect.

La diffÃ©rence entre "Faster R-CNN" et SSD est quâ€™avec R-CNN on rÃ©alise une classification sur chacune des 2000 fenÃªtres gÃ©nÃ©rÃ©es par lâ€™algorithme de recherche ciblÃ©e, alors quâ€™avec SSD on cherche Ã  prÃ©dire la classe ET la fenÃªtre de lâ€™objet en mÃªme temps. Cela rend SSD plus rapide que "Faster R-CNN", mais Ã©galement moins prÃ©cis.

Dans le tableau du site [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md), les performances des diffÃ©rents rÃ©seaux sont exprimÃ©es en _COCO mAP (Mean Average Precision)_, mÃ©trique couramment utilisÃ©e pour mesurer la prÃ©cision dâ€™un modÃ¨le de dÃ©tection dâ€™objets. Elle consiste Ã  mesurer la proportion de dÃ©tections rÃ©ussies sur des images dÃ©jÃ  annotÃ©es du dataset COCO (Common Object in CONtext)
qui contient 200 000 images annotÃ©es avec 80 objets diffÃ©rents. Cette mesure sert de rÃ©fÃ©rence pour comparer la prÃ©cision de diffÃ©rentes architectures de dÃ©tection dâ€™objets (plus dâ€™informations sur _mAP_ dans la lecture [2]).


ğŸ“¥ Pour le travail de dÃ©tection des faces des cubes dans les images fournies par la camÃ©ra du robot Ergo Jr tu peux tÃ©lÃ©charger le rÃ©seau `Faster R-CNN ResNet50 V1 640x640` sur le site [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) (~203 Mo).

Une fois tÃ©lÃ©chargÃ©e, il faut extraire l'archive TGZ au bon endroit dans l'arborescence de travail :
```bash
# From within tod_tf2/
(tf2) jlc@pikatchou $ tar xvzf ~/TÃ©lÃ©chargements/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz -C pre_trained
```
puis crÃ©er le dossier correspondant `faster_rcnn_resnet50_v1_640x640_coco17_tpu-8` dans le dossier `training/faces_cubes` :
```bash	
# From within tod_tf2/
(tf2) jlc@pikatchou $ mkdir training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8
```
On vÃ©rifie :
```bash
# From within tod_tf2/
(tf2) jlc@pikatchou $ tree -d pre_trained
pre_trained
â””â”€â”€ faster_rcnn_resnet50_v1_640x640_coco17_tpu-8
    â”œâ”€â”€ checkpoint
    â””â”€â”€ saved_model
        â””â”€â”€ variables
        
(tf2) jlc@pikatchou $ tree -d training
training
â””â”€â”€ faces_cubes
    â””â”€â”€ faster_rcnn_resnet50_v1_640x640_coco17_tpu-8
```

## 5. CrÃ©er les donnÃ©es pour l'apprentissage supervisÃ©

Cette Ã©tape du travail comporte cinq tÃ¢ches :

1. CrÃ©er des images avec la camÃ©ra du robot -> fichiers \*.jpg, \*.png
2. Annoter les images avec le logiciel `labelImg` -> fichiers \*.xml
3. Convertir les fichiers annotÃ©s XML au format CSV
4. Convertir les fichiers annotÃ©s CSV au format _tensorflow record_
5. CrÃ©er le fichier `label_map.pbtxt` qui contient les labels des objets Ã  reconnaÃ®tre.


### 5.1 CrÃ©er les images avec la camÃ©ra du robot  

Les images des faces des cubes peuvent Ãªtre obtenues en utilisant le service ROS `/get_image` proposÃ© par le robot Poppy Ergo Jr.

image001.png               |  image002.png
:-------------------------:|:-------------------------:
![image1](img/image000.png)   |  ![image2](img/image001.png)


ğŸ¤– Rappels : lancement du ROS Master et des services ROS sur le robot :
 
* allumer le robot Poppy Ergo Jr,
* se connecter sur la carte RPi du robot : `ssh pi@poppy.local` (mdp: `raspberry`) 
* âœ… vÃ©rifier que `ROS_MASTER_URI` pointe bien vers `poppy.local:11311` :
```bash
(tf2) jlc@pikatchou: $ ssh pi@poppy.local
pi@poppy.local password:
...

pi@poppy:~ $ env|grep ROS_MASTER
ROS_MASTER_URI=http://poppy.local:11311
```	
* si `ROS_MASTER_URI` n'est pas bon, Ã©dite le fichier `~/.bashrc` du robot, mets la bonne valeur et tape `source ~\.bashrc`...
* Lance le ROS Master et les services ROS sur le robot avec la commande : 
```bash
pi@poppy:~ $ roslaunch poppy_controllers control.launch
...
```

ğŸ’» Et maintenant dans un terminal sur ton PC, avec l'EVP `(tf2)` dÃ©sactivÃ© :
* âœ… vÃ©rifie que `ROS_MASTER_URI` pointe bien vers `poppy.local:11311` :
```bash
(tf2) jlc@pikatchou: $ env|grep ROS_MASTER
ROS_MASTER_URI=http://poppy.local:11311
```	
* si `ROS_MASTER_URI` n'est pas bon, Ã©dite le fchier `~/.bashrc`, mets la bonne valeur et tape `source ~\.bashrc`...


ğŸ Tu peux utiliser le programme Python `get_image_from_robot.py` du dossier `tod_tf2` pour enregistrer les images des cubes dans des fichiers nommÃ©es `imagesxxx.png` (`xxx` = `001`, `002`...). 
Un appui sur une touche clavier permet de passer d'une image Ã  l'autre, un appui sur la touche `Q` permet de quitter le programme :

```python
import cv2, rospy
from poppy_controllers.srv import GetImage
from cv_bridge import CvBridge

i=1
while True:
    get_image = rospy.ServiceProxy("get_image", GetImage)
    response  = get_image()
    bridge    = CvBridge()
    image     = bridge.imgmsg_to_cv2(response.image)
    cv2.imwrite(f"image{i:03d}.png", image)
    cv2.imshow("Poppy camera", image)
    key = cv2.waitKey(0)
    if key==ord('q') or key==ord("Q"): break
    cv2.destroyAllWindows()
    i += 1
cv2.destroyAllWindows()
```

ğŸ“  En cas de conflit grave "ROS / EVP tf2 / PyQT" en utilisant le programme `get_image_from_robot.py` tu peux dÃ©sactiver temporairement l'EVP tf2 :
* soit en lanÃ§ant un nouveau terminal,
* soit en tapant la commande `conda deactivate`


Chaque Ã©quipe doit faire une dizaine d'images en variant les faces des cubes visibles, puis les images pourront Ãªtre partagÃ©es sur un serveur pour servir Ã  toutes les Ã©quipes.

Une fois collectÃ©es toutes les images, il faut mettre environ 90 % des images dans le dossier `images\faces_cubes\train` et le reste dans le dossier `images\faces_cubes\test`.

### 5.2 Annoter les images avec le logiciel labelImg

L'annotation des images peut Ãªtre faite de faÃ§on trÃ¨s simple avec le logiciel `labelImg`.
Câ€™est une Ã©tape du travail qui prend du tempsÂ et qui peut Ãªtre rÃ©alisÃ©e Ã  plusieurs en se rÃ©partissant les images Ã  annoter...

L'installation du module Python `labelImg` faite dans l'EVP `tf2` (cf section 2.) permet de lancer le logiciel `labelImg` en tapantÂ :
```bash
(tf2) jlc@pikatchou:~ $ labelImg
```

Utilise les boutons [Open Dir] et [Change Save Dir] pour te positionner la lecture ET l'Ã©criture des fichiers dans le dossier `images/face_cubes/train/`.<br>
La premiÃ¨re image est automatiquement chargÃ©e dans l'interface graphique :

![labelImg_2.png](img/labelImg_2.png)

Pour chaque image, tu dois annoter les objets Ã  reconnaÃ®tre :
* avec le bouton [Create RectBox], tu entoures une face d'un cube,
* la boÃ®te des labels s'ouvre alors et tu dois Ã©crire le blabel `one` ou `two` en fonction de la face entourÃ©e,
* itÃ¨re le processus pour chacune des faces de cubes prÃ©sente dans l'image...

    premiÃ¨re face          |  deuxiÃ¨me face            |  fin
:-------------------------:|:-------------------------:|:-------------------------:
![1](img/labelImg_3.png)   |  ![2](img/labelImg_4.png) | ![3](img/labelImg_5.png)

* quand c'est fini, tu cliques sur le bouton [Save] et tu passes Ã  l'image suivante avec le bouton [Next Image].
* Une fois toutes les images annotÃ©es, utilise les boutons [Open Dir] et [Change Save Dir] pour annoter de la mÃªme faÃ§on les images de test du dossier `images/face_cubes/test/`.

### 5.3 Convertir les fichiers XML annotÃ©s au format CSV

Cette Ã©tape permet de synthÃ©tiser dans un fichier CSV unique les donnÃ©es dâ€™apprentissage contenues dans les diffÃ©rents fichiers XML crÃ©es Ã  lâ€™Ã©tape d'annotation. 
Le programme `xml_to_csv_tt.py` permet de gÃ©nÃ©rer les deux fichiers CSV correspondant aux donnÃ©es dâ€™apprentissage et de test. <br>
Depuis le dossier `tod_tf2` tape la commande suivanteÂ :

```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ python xml_to_csv_tt.py -p faces_cubes
Successfully converted xml data in file <images/faces_cubes/train_labels.csv>.
Successfully converted xml data in file <images/faces_cubes/test_labels.csv>.
```
Les fichiers `train_labels.csv` et `test_labels.csv` sont crÃ©Ã©s dans le dossier  `images/faces_cubes/`.

### 5.4 Convertir les fichiers CSV annotÃ©s au format _tfrecord_

Pour cette Ã©tape, on utilise le programme `generate_tfrecord_tt.py`.<br>
Depuis le dossier `tod_tf2` tape la commande :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ python generate_tfrecord_tt.py --project faces_cubes
Successfully created the TFRecord file: ./training/faces_cubes/train.record
Successfully created the TFRecord file: ./training/faces_cubes/test.record
```
Avec cette commande tu viens de crÃ©er les 2 fichiers `train.record` et `test.record` dans le dossier `training/faces_cubes`Â : ce sont les fichiers qui serviront pour lâ€™entraÃ®nement et l'Ã©valuation du rÃ©seau.

### 5.5 CrÃ©er le fichier label_map.pbtxt
 
La derniÃ¨re Ã©tape consiste a crÃ©er le fichier `label_map.pbtxt` dans le dossier `training/faces_cubes`. 

Ce fichier dÃ©crit la Â«Â carte des labelsÂ Â» (_label map_) nÃ©cessaire Ã  lâ€™entraÃ®nement du rÃ©seau. 
La carte des labels permet de connaÃ®tre lâ€™ID (nombre entier) associÃ© Ã  chaque Ã©tiquette (_label_) identifiant les objets Ã  reconnaÃ®tre. La structure type du fichier est la suivanteÂ :


	 item {
	   id: 1
	   name: 'objet_1'
	 }
	 item {
	   id: 2
	   name: 'objet_2'
	 }
	 ...

Pour le projet `face_cubes`, le contenu du fichier `training/faces_cubes/label_map.pbtxt` Ã  crÃ©er est :

	 item {
	   id: 1
	   name: 'one'
	 }
	 item {
	   id: 2
	   name: 'two'
	 }

## 6. Lancer l'entraÃ®nement supervisÃ© du rÃ©seau prÃ©-entraÃ®nÃ©

Ce travail se dÃ©compose en plusieurs Ã©tapes :

1. Modifier le fichier de configuration du rÃ©seau prÃ©-entraÃ®nÃ© pour dÃ©crire la configuration d'entraÃ®nement.
2. Lancer l'entraÃ®nement supervisÃ©.
3. Exporter les poids du rÃ©seau entrainÃ© dans un format utilisable.

### 6.1 Modifier le fichier de configuration

Câ€™est la derniÃ¨re Ã©tape avant de lancer lâ€™entraÃ®nementâ€¦

* Le fichier de configuration `pipeline.config` prÃ©sent dans le dossier `pre_trained/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8` doit Ãªtre copiÃ© dans le dossier cible `training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8`. 

* Il faut ensuite modifier les paramÃ¨tres du fichier `training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8` pour les adpater Ã  l'entraÃ®nement :

|nÂ° | paramÃ¨tre                     | Description                                                            | Valeur initiale  | valeur Ã  donner |  explication                    |
|:--|:------------------------------|:-----------------------------------------------------------------------|:----------------:|:---------------:|:--------------------------------|
|010| `num_classes`                 | nombre de classe d'objets                                              | 90               | 2               | les deux classes 'one' et 'two' |
|077| `max_detections_per_class`    | nombre max de dÃ©tection par classe                                     | 100              | 4               | 4 cubes          | 
|078| `max_total_detections`        | nombre max total de dÃ©tections                                         | 100              | 4               | 4 cubes          | 
|093| `batch_size`                  | nombre d'images Ã  traiter en lot avant mise Ã  jour des poids du rÃ©seau | 64               | 1, 2,...        | une valeur trop Ã©levÃ©e risque de faire dÃ©passer la capacitÃ© mÃ©moire RAM de ta machine... Ã  rÃ©gler en fonction de la quantitÃ© de RAM de ta machine.  |
|097| `num_steps`                   | Nombre max d'itÃ©rations d'entraÃ®nement                                 | 25000             | 1000           | une valeur trop grande donne des temps de calcul prohibitifs et un risque de sur-entraÃ®nement 
|113| `fine_tune_checkpoint`        | chemin des fichiers de sauvegarde des poids du rÃ©seau prÃ©-entraÃ®nÃ©     | "PATH_TO_BE_<br>CONFIGURED" | "pre_trained/faster_rcnn_resnet50_v1_<br>640x640_coco17_tpu-8/checkpoint/ckpt-0" | se termine par `/ckpt-0` qui est le prÃ©fixe des fichiers dans le dossier `.../checkpoint/` |
|114| `fine_tune_checkpoint_type`   | Choix de l'algorithme : "classification" ou "detection"                | "classification"| "detection"  | on veut faire de la detection d'objets |
|120| `max_number_of_boxes`         | Nombre max de boÃ®tes englobantes  dans chaque image                    | 100               | 4               | les faces des cubes sur une image |
|122| `use_bfloat16`                | `true` pour les architectures TPU, `false` pour CPU                    | true              | false           | |
|126| `label_map_path`              | chemin du fichier des labels                                           | "PATH_TO_BE_<br>CONFIGURED" | "training/faces_cubes/label_map.pbtxt" | utilisÃ© pour l'entraÃ®nement |
|128| `input_path`                  | fichier des donnÃ©es d'entrÃ©e d'entraÃ®nement au format `tfrecord`       | "PATH_TO_BE_<br>CONFIGURED" | "training/faces_cubes/train.record"    | utilisÃ© pour l'entraÃ®nement |
|139| `label_map_path`              | chemin du fichier des labels                                           | "PATH_TO_BE_<br>CONFIGURED" | "training/faces_cubes/label_map.pbtxt" | utilisÃ© pour l'Ã©valuation|
|128| `input_path`                  | fichier des donnÃ©es d'entrÃ©e de test au format `tfrecord`              | "PATH_TO_BE_<br>CONFIGURED" | "training/faces_cubes/test.record"    | utilisÃ© pour l'Ã©valuation|

## 6.2 Lancer l'entraÃ®nement

* Copie le fichier `models\research\object_detection\model_main_tf2.py` dans la racine `tod_tf2`.
* Tape la commande :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou $ python model_main_tf2.py --model_dir=training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint1  --pipeline_config_path=training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/pipeline.config
```
Les fichiers des poids entraÃ®nÃ©s seront Ã©crits dans le dossier `.../faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint1` : si tu relances l'entraÃ®nement, tu peux utiliser `.../checkpoint2`, `.../checkpoint3` pour sÃ©parer des essais successifs.

Le programme Python lancÃ© est trÃ¨s verbeux...<br>
au bout d'un temps qui peut Ãªtre assez long (plusieurs minutes avec un petit CPU), les logs de l'entraÃ®nement apparaissent Ã  l'Ã©cran :

	...
	...
	W0507 00:24:41.010936 140206908888832 deprecation.py:531] From /home/jlc/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.
	Instructions for updating:
	Use fn_output_signature instead
	INFO:tensorflow:Step 100 per-step time 22.002s loss=0.825
	I0507 01:01:11.942076 140208909420352 model_lib_v2.py:676] Step 100 per-step time 22.002s loss=0.825
	INFO:tensorflow:Step 200 per-step time 20.926s loss=0.813
	I0507 01:36:04.090147 140208909420352 model_lib_v2.py:676] Step 200 per-step time 20.926s loss=0.813
	INFO:tensorflow:Step 300 per-step time 20.803s loss=0.801
	I0507 02:10:44.351419 140208909420352 model_lib_v2.py:676] Step 300 per-step time 20.803s loss=0.801
	INFO:tensorflow:Step 400 per-step time 20.946s loss=0.812
	I0507 02:45:38.927271 140208909420352 model_lib_v2.py:676] Step 400 per-step time 20.946s loss=0.812
	INFO:tensorflow:Step 500 per-step time 20.960s loss=0.794
	I0507 03:20:34.990385 140208909420352 model_lib_v2.py:676] Step 500 per-step time 20.960s loss=0.794
	INFO:tensorflow:Step 600 per-step time 21.045s loss=0.802
	I0507 03:55:39.516442 140208909420352 model_lib_v2.py:676] Step 600 per-step time 21.045s loss=0.802
	INFO:tensorflow:Step 700 per-step time 20.863s loss=0.786
	I0507 04:30:25.868283 140208909420352 model_lib_v2.py:676] Step 700 per-step time 20.863s loss=0.786
	INFO:tensorflow:Step 800 per-step time 20.744s loss=0.799
	I0507 05:05:00.163027 140208909420352 model_lib_v2.py:676] Step 800 per-step time 20.744s loss=0.799
	INFO:tensorflow:Step 900 per-step time 20.825s loss=0.837
	I0507 05:39:42.691898 140208909420352 model_lib_v2.py:676] Step 900 per-step time 20.825s loss=0.837
	INFO:tensorflow:Step 1000 per-step time 20.789s loss=0.778
	I0507 06:14:21.503472 140208909420352 model_lib_v2.py:676] Step 1000 per-step time 20.789s loss=0.778

Dans l'exemple ci-dessus, on voit des logs tous les 100 pas, avec environ 20 secondes par pas, soit environ 35 minutes entre chaque affichage et environ 6h de calcul pour les 1000 pas.

En cas d'arrÃªt brutal du programme avec le message "Processus arrÃªtÃ©", ne pas hÃ©siter Ã  diminer la valeur du paramÃ¨tre `batch_size` jusquÃ  2 voire 1 si nÃ©cessaire.... <br>
MÃªme avec un `batch_size` de 2, le processus Python peut nÃ©cessiter jusqu'Ã  2 ou 3 Go de RAM pour lui tout seul, ce qui peut mettre certains portables en difficultÃ©...

Une fois l'entraÃ®nement terminÃ© tu peux analyser les statistiques d'entraÃ®nement avec `tensorboard` en tapant la commande :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ tensorboard --logdir=training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint1/train
Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
TensorBoard 2.4.0 at http://localhost:6006/ (Press CTRL+C to quit)
...
```
`tensorflow` lance un serveur HHTP en local sur ta machine, et tu peux ouvrir la page `http://` avec un navigateur pour voir les courbes d'analyse en faisant CTRL + clic avec le curseur de la souris positionnÃ© sur le mot `http://localhost:6006/` :

![tensorflow](img/tensorboard.png)

Le logiciel tensorboard permet d'examiner l'Ã©volution de statistiques caractÃ©ristiques de l'apprentissage.

### 6.3 Exporter les poids du rÃ©seau entraÃ®nÃ©

On utilise le script Python `exporter_main_v2.py` du dossier `models/reasearch/object_detection/` pour extraire le __graph d'infÃ©rence__ entraÃ®nÃ© et le sauvegarder dans un fichier `saved_model.pb` qui pourra Ãªtre rechargÃ© ultÃ©rieurement pour exploiter le rÃ©seau entraÃ®neÃ© :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou $ cp models/research/object_detection/exporter_main_v2.py .
(tf2) jlc@pikatchou $ python exporter_main_v2.py --input_type image_tensor --pipeline_config_path training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/pipeline.config --trained_checkpoint_dir training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint1 --output_directory training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saved_model1
```
Le script Python crÃ©Ã© le fichier `saved_model.pb` dans le dossier `.../faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saved_model1/saved_model` :

```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ tree training/
training/
â””â”€â”€ faces_cubes
    â””â”€â”€ faster_rcnn_resnet50_v1_640x640_coco17_tpu-8
        â”œâ”€â”€ checkpoint1
        â”‚Â Â  â”œâ”€â”€ checkpoint
        â”‚Â Â  â”œâ”€â”€ ckpt-1.data-00000-of-00001
        â”‚Â Â  â”œâ”€â”€ ckpt-1.index
        â”‚Â Â  â””â”€â”€ train
        â”‚Â Â      â””â”€â”€ events.out.tfevents.1620391994.pikatchou.30554.1504.v2
        â”œâ”€â”€ pipeline.config
        â”œâ”€â”€ saved_model1
        â”‚Â Â  â”œâ”€â”€ checkpoint
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ checkpoint
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt-0.data-00000-of-00001
        â”‚Â Â  â”‚Â Â  â””â”€â”€ ckpt-0.index
        â”‚Â Â  â”œâ”€â”€ pipeline.config
        â”‚Â Â  â””â”€â”€ saved_model
        â”‚Â Â      â”œâ”€â”€ assets
        â”‚Â Â      â”œâ”€â”€ saved_model.pb
        â”‚Â Â      â””â”€â”€ variables
        â”‚Â Â          â”œâ”€â”€ variables.data-00000-of-00001
        â”‚Â Â          â””â”€â”€ variables.index
```


## 7. Ã‰valuation du rÃ©seau entraÃ®nÃ©

On va vÃ©rifier que le rÃ©seau entraÃ®nÃ© est bien capable de dÃ©tecter les faces des cubes en discriminant correctement les numÃ©ros Ã©crits sur les faces.

Le script Python `plot_object_detection_saved_model.py` permet d'exploiter le rÃ©seau entraÃ®nÃ© sur des images, les arguments sont :
* `-p` : le nom du projet
* `-m` : le chemin du dossier `.../saved/` contenant les fichiers des poids du rÃ©seau entraÃ®nÃ©
* `-i` : le chemin du dossier des images ou le chemin du fichier image Ã  analyser
* `-n` : le nombre max d'objets Ã  dÃ©tecter
* `-t` : le seuil (_threshold_) de dÃ©tection exprimÃ© en % (optionnel, valeur par dÃ©faut : 50 %).

Par exemple pour faire la dÃ©tection des cubes des images de test avec le rÃ©seau qu'on vient d'entraÃ®ner :

```bash
# From within tod_tf2
(tf2) jlc@pikatchou: $ python plot_object_detection_saved_model.py -p faces_cubes -s training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saved_model1/saved_model -i images/faces_cubes/test/ -n 4

Loading model...Done! Took 11.77 seconds

Running inference for images/faces_cubes/test/image016.png... [2 1 1 2]
[0.999408   0.99929774 0.9985869  0.99794155]
[[0.4046488  0.13016616 0.6338345  0.31058723]
 [0.40798646 0.56277716 0.63340956 0.7373474 ]
 [0.40612057 0.3360289  0.63908    0.5120028 ]
 [0.40730068 0.7692113  0.6340802  0.9632611 ]]

Running inference for images/faces_cubes/test/image018.png... [2 2 1 1]
[0.9995958  0.99956626 0.99756575 0.9960402 ]
[[0.4140944  0.62948036 0.6388739  0.7997428 ]
 [0.41462958 0.40451866 0.6399791  0.5834095 ]
 [0.41448513 0.19922832 0.63370967 0.36855492]
 [0.40865567 0.         0.63875306 0.16509269]]

Running inference for images/faces_cubes/test/image019.png... [2 2 1 1]
[0.99956614 0.99939644 0.9977343  0.99497354]
[[0.41152024 0.43164197 0.6439534  0.6055011 ]
 [0.41524586 0.65218127 0.6408151  0.8317957 ]
 [0.40619218 0.20989983 0.6326463  0.40074167]
 [0.40634462 0.         0.6389088  0.18785618]]

Running inference for images/faces_cubes/test/image017.png... [2 2 1 1]
[0.999482   0.99903905 0.9982924  0.99810815]
[[0.4101084  0.70229053 0.63589627 0.8758344 ]
 [0.4053984  0.05132582 0.6378698  0.23081933]
 [0.41257906 0.49104023 0.63610333 0.66249573]
 [0.40499112 0.29253355 0.63419634 0.46947986]]
```
Pour chaque image traitÃ©e on affiche ici :
* la liste des 4 labels des objets trouvÃ© (1 ou 2)
* la liste des 4 probabilitÃ©s de dÃ©tection des objets
* la liste des 4 jeux de coordonnÃ©es normalisÃ©es des boÃ®tes englobantes [ y x coin haut gauche puis y x coin bas droit]. 

Les images produites sont :

|   image016.png           |   image018.png               |            image019.png    |    image017.png
:-------------------------:|:----------------------------:|:--------------------------:|:------------------------------:
![1](img/infere_img01.png) |  ![2](img/infere_img02.png)  | ![3](img/infere_img03.png) | ![4](img/infere_img04.png)
 

## 8. IntÃ©gration

Une fois le rÃ©seau entraÃ®nÃ© et Ã©valuÃ©, si les rÃ©sultats sont bons, "il ne reste plus qu'Ã " crÃ©er le fichier `nn.py` pour rÃ©aliser les traitements nÃ©cessaires Ã  l'exploitation du rÃ©seau entraÃ®nÃ© pour ton projet : le but est d'intÃ©grer le rÃ©seau de neurones `nn`  dans le contexte ROS :

![intÃ©gration ROS](../../integration/ergo-tb-tf2/img/UML_integration.png)
 
1. Attendre que le paramÃ¨tre ROS  `takeImage` passe Ã  `True` et le remettre Ã  `False`
3. Obtenir le fichier de l'image prise par la camÃ©ra du robot grÃ¢ce au service ROS `/get_image`
4. Traiter l'image pour obtenir les labels et les boÃ®tes englobantes des faces des cubes (penser Ã  remettre les cubes dans le bon ordre...)
5. Et pour chaque cube : donner au paramÃ¨tre ROS`label` la valeur du label du cube, mettre le paramÃ¨tre ROS `RobotReady` Ã  `False` et attendre que le paramÃ¨tre rOS `RobotReady` repasse Ã  True


