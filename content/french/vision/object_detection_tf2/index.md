---
title: "D√©tection d'objets avec tensorflow2"
menu:
  main:
    name: "D√©tection d'objets avec tf2"
    weight: 3
    parent: "vision"
---

Dans cette section nous proposons d'utiliser l'API __Tensorflow Object Detection__ (_a.k.a_ TOD) qui propose :
* une collection de r√©seaux d√©j√† entra√Æn√©s sp√©cialement con√ßus pour pour la d√©tection d'objets dans des images (__Object Detection__),
* le m√©canisme de _transfert learning_ pour continuer l'entra√Ænement des r√©seaux pr√©-entra√Æn√©s avec nos propres images labellis√©es, 
pour obtenir la d√©tection des objets qui nous int√©ressent.

Contrairement √† la strat√©gie de __Classification__ pr√©sent√©e dans dans la section [Classification tf2](https://learn.e.ros4.pro/fr/vision/classification_tf2/), 
la __D√©tection d'objets__ permet de trouver directement les bo√Ætes englobantes des objets "face de cube avec un 1" et "face de cube avec un 2".

Cette approche √©vite la phase de traitement d'image classique pour extraire puis classifier les images des faces des cubes. 

Le traitement d'image bas√© sur une approche traditionnelle de manipulation des pixels de l'image (seuillage, extraction de contour, segmentation...) reste assez fragile : en particulier il est  sensible √† la luminosit√©, √† la pr√©sence ou non d'un fond noir... Un avantage attendu de l'approcje Object Detection est de fournir directement les bo√Ætes englobantes des faces des cubes sans passer par une √©tapde de traitement d'image.

## Pr√©requis

* Bonne compr√©hension de Python et numpy
* Une premi√®re exp√©rience des r√©seaux de neurones est souhaitable.

L'entra√Ænement des r√©seaux de neurones avec le module `tensorflow` se fera de pr√©f√©rence dans un environnement virtuel Python (EVP) qui permet de travailler dans un environnement Python d√©di√©.
Vous pouvez vous r√©ferrer √† la [FAQ Python : environnement virtuel](https://learn.e.ros4.pro/fr/faq/python_venv/) 

## 1. Documentation

Documentation g√©n√©rale sur numpy :
* [Numpy cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)
* [NumPy quickstart](https://numpy.org/devdocs/user/quickstart.html)

Documentation sur l'_API TOD_ pour `tensorflow2` :
* Le tutoriel officiel complet : [TensorFlow 2 Object Detection API tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html)
* Le d√©p√¥t git : [models/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)

Ce tutoriel peut √™tre consult√© pour aller chercher des d√©tails qui ne sont pas d√©velopp√©s dans l'activit√© propos√©e, mais il est pr√©ferrable de suivre 
les indications du paragraphe suivant pour installer une version r√©cente de tensorflow2. 

## 2. Installation de l'API TOD

### 2.1 Clonage du d√©p√¥t `tensorflow/models`

Cr√©√© un dossier sp√©cifique pour le travail avec l'API TOD en clonant le d√©p√¥t github `cjlux/tod_tf2.git` :

```bash
(tf2) jlc@pikatchou~$ cd <quelque_part>   # choisis le r√©pertoire o√π cloner `tod_tf2.git`, par exemple : ~/catkin_ws/
(tf2) jlc@pikatchou~$ git clone https://github.com/cjlux/tod_tf2.git
```
Le clonage cr√©√© le dossier `tod_tf2` contenant des scripts Python qui seront utilis√©s plus tard. Ce dossier est la racine du projet.<br>
Dans le dossier `tod_tf2` clone le d√©p√¥t github `tensorflow/models`¬†:
```bash
(tf2) jlc@pikatchou~$ cd tod_tf2
(tf2) jlc@pikatchou~$ git clone https://github.com/tensorflow/models.git
```

Tu obtiens un dossier `models` : l‚ÄôAPI TOD est dans le dossier `models/research/object_detection` :
```bash	
(tf2) jlc@pikatchou~$ tree -d -L 2 .
.
‚îî‚îÄ‚îÄ models
    ‚îú‚îÄ‚îÄ community
    ‚îú‚îÄ‚îÄ official
    ‚îú‚îÄ‚îÄ orbit
    ‚îî‚îÄ‚îÄ research
```	

Compl√®te ton installation avec quelques paquets Python utiles pour le travail avec l'API TOD :
```bash
(tf2) jlc@pikatchou $ conda install cython contextlib2 pillow lxml
(tf2) jlc@pikatchou $ pip install labelimg rospkg
```
Mets √† jour la variable d‚Äôenvironnement `PYTHONPATH` en ajoutant √† la fin du fichier ~/.bashrc les deux lignes¬†:
```bash
export TOD_TF2="<chemin absolu du dossier tod_tf2>"
export PYTHONPATH=$TOD_TF2/models:$TOD_TF2/models/research:$PYTHONPATH
```
remplace `"<chemin absolu du dossier tod_tf2>"` par le chemin absolu du dossier `tod_tf2` sur ta machine.

Lance un nouveau terminal pour activer le nouvel environnement shell¬†; tout ce qui suit sera fait dans ce nouveau terminal.

### 2.2 Installer les outils `protobuf`

L‚ÄôAPI native TOD utilise des fichiers `*.proto`¬†pour la configuration des mod√®les et le stockage des param√®tres d‚Äôentra√Ænement. 
Ces fichiers doivent √™tre traduits en fichiers `*.py` afin que l‚ÄôAPI Python puisse fonctionner correctement.  

Du dois installer en premier la commande `protoc`¬†:
```bash
(tf2) jlc@pikatchou $ sudo apt install protobuf-compiler
```
Tu peux ensuite te positionner dans le dossier `tod_tf2/models/research` et taper¬†:
```bash
# From tod_tf2/models/research/
(tf2) jlc@pikatchou $ protoc object_detection/protos/*.proto  --python_out=.
```
Cette commande travaille de fa√ßon muette.

### 2.3 Installer l'API COCO

COCO est une banque de donn√©es destin√©e √† alimenter les algorithmes de d√©tection d‚Äôobjets, de segmentation‚Ä¶ voir [cocodataset.org](https://cocodataset.org) pour les tutoriels, publications‚Ä¶ 

üíª Pour installer l‚ÄôAPI Python de COCO, clone le site cocoapi dans le dossie `/tmp`, tape la commande `make` dans le dossier `cocoapi/PythonAPI`, puis recopie le dossier `pycococtools` dans `models/research/`¬†:
```bash
(tf2) jlc@pikatchou~$ cd /tmp
(tf2) jlc@pikatchou~$ git clone  https://github.com/cocodataset/cocoapi.git
(tf2) jlc@pikatchou~$ cd cocoapi/PythonAPI/
(tf2) jlc@pikatchou~$ make
(tf2) jlc@pikatchou~$ cp -r pycocotools/ <chemin absolu du dossier tod_tf2>/models/research/
```
### 2.4 Finalisation 

üíª Pour finir l'installation, place-toi dans le dossier  `models/research/` et tape les commandes :
```bash
# From tod_tf2/models/research/
(tf2) jlc@pikatchou $ cp object_detection/packages/tf2/setup.py .
(tf2) jlc@pikatchou $ python setup.py build
(tf2) jlc@pikatchou $ pip install .
```

### 2.5 Tester l'installation de l'API TOD

üíª Pour tester ton installation de l‚ÄôAPI TOD, place-toi dans le dossier `models/research/` et tape la commande¬†:
```bash	
# From within tod_tf2/models/research/
(tf2) jlc@pikatchou~$ python object_detection/builders/model_builder_tf2_test.py
```
Le programme d√©roule toute une s√©rie de tests et doit se terminer par un OK¬†:

	...
	[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size
	[ RUN      ] ModelBuilderTF2Test.test_session
	[  SKIPPED ] ModelBuilderTF2Test.test_session
	[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s
	I0505 18:19:38.639148 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor
	[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s
	I0505 18:19:38.640017 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture
	[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s
	I0505 18:19:38.641987 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor
	----------------------------------------------------------------------
	Ran 21 tests in 53.105s

	OK (skipped=1)

üíª Pour finir, tu peux v√©rifier l‚Äôinstallation en utilisant un cahier IPython de l'API TOD ¬†: dans le dossier `models/research/object_detection` lance la commande jupyter notebook et charge le cahier `colab/object_detection_tutorial.ipynb` : 

* ‚ö†Ô∏è Avant d'ex√©cuter les cellules du notebook, il faut corriger une erreur dans le fichier `.../miniconda3/envs/tf2/lib/python3.8/site-packages/object_detection/utils/ops.py`, ligne 825 :
remplacer `tf.uint8` par `tf.uint8.as_numpy_dtype`
* ‚ö†Ô∏è Attention √† ne pas exc√©cuter les cellules du notebook comportant les commandes `!pip install ...` ou qui contiennent la _magic chain_ `%%bash`¬†:	

![notebook_test_TOD.png](img/notebook_test_TOD.png)

Ex√©cute les cellules une √† une, tu ne dois pas avoir d‚Äôerreur :

* La partie "__Detection__" (qui dure quelques secondes ou quelques minutes suivant ton CPU‚Ä¶) utilise le r√©seau pr√©-entra√Æn√© `ssd_mobilenet_v1_coco_2017_11_17` pour d√©tecter des objets dans les   images¬†de test :	

![notebook_test_TOD_image1et2.png](img/notebook_test_TOD_image1et2.png)

* La partie "__Instance Segmentation__" utilise le r√©seau pr√©-entra√Æn√© `mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28` pour d√©tecter les objets et leurs masques, par exemple :

![notebook_test_TOD_image-mask1.png](img/notebook_test_TOD_image-mask1.png)

La suite du travail se d√©compose ainsi :
* Cr√©er l'arborescence de travail
* T√©l√©charger le r√©seau pr√©-entra√Æn√©
* Cr√©er la banque d'images labellis√©es pour l'entra√Ænement supervis√© du r√©seau choisi
* Entra√Æner le r√©seau avec la banque d'images labellis√©es.
* √âvaluer les inf√©rences du r√©seau avec les images de test
* Int√©grer de l'exploitation du r√©seau dans l'environnement ROS.

## 3. Cr√©er l'arborescence de travail

L'arborescence g√©n√©rique de travail propos√©e pour cette activit√© est la suivante :

	tod_tf2
	‚îú‚îÄ‚îÄ images
	‚îÇ   ‚îî‚îÄ‚îÄ<project>
	‚îÇ       ‚îú‚îÄ‚îÄ test
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îú‚îÄ‚îÄ train
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îî‚îÄ‚îÄ *.csv
	‚îú‚îÄ‚îÄ pre_trained
	‚îÇ	‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îú‚îÄ‚îÄ training
	‚îÇ   ‚îú‚îÄ‚îÄ<project>
	‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îÇ   ‚îú‚îÄ‚îÄ train.record
	‚îÇ   ‚îú‚îÄ‚îÄ test.record
	‚îÇ   ‚îî‚îÄ‚îÄ label_map.txt
	‚îî‚îÄ‚îÄ models
	    ‚îî‚îÄ‚îÄ research
	        ‚îî‚îÄ‚îÄ object_detection
	
	

* Le dossier `images/` contient un dossier pour chaque projet, avec dedans :
	* les dossiers `test` et `train` qui contiennent chacun :
		* les images (\*.png, \*.jpg) √† analyser,
		* les fichiers d'annotation (\*.xml) faits avec le logiciel `labelImg` : ils donnent, pour chacun des objets d'une image, les coordonn√©es de la bo√Æte englobante et le label de l'objet.
	* les fichiers d'annotation \*.cvs (convertis au format CSV), qui seront √† leur tour convertis au format _tensorflow record_.
* Le dossier `pre_trained/` contient un sous-dossier pour chacun des r√©seaux pr√©-entrain√©s utilis√©.
* le dossier `training/` contient  √©galement un dossier pour chaque projet, avec dedans :
	* un dossier pour chacun des r√©seaux pr√©-entrain√©s utilis√© : c'est dans ce dossier que seront stock√©s les fichiers des poids du r√©seau entra√Æn√©,
	* les fichiers `train.reccord`  et `test.reccord` : contiennent les donn√©es labelis√©es d'entra√Ænement et de test converties du format CSV au format _tensorflow record_,
	* le fichier `label_map.txt` : liste les labels correpondants aux objets √† d√©tecter.
	
Pour la d√©tection des faces des cubes dans les images faites par le robot Poppy Ergo Jr, le dossier `<project>` sera nomm√© `faces_cubes`, ce qui donne l'arborescence de travail :

	tod_tf2
	‚îú‚îÄ‚îÄ images
	‚îÇ   ‚îî‚îÄ‚îÄ faces_cubes
	‚îÇ       ‚îú‚îÄ‚îÄ test
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îú‚îÄ‚îÄ train
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îî‚îÄ‚îÄ *.csv
	‚îú‚îÄ‚îÄ pre_trained
	‚îÇ	‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îú‚îÄ‚îÄ training
	‚îÇ   ‚îú‚îÄ‚îÄ faces_cubes
	‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îÇ   ‚îú‚îÄ‚îÄ train.record
	‚îÇ   ‚îú‚îÄ‚îÄ test.record
	‚îÇ   ‚îî‚îÄ‚îÄ label_map.txt
	‚îî‚îÄ‚îÄ models
	    ‚îî‚îÄ‚îÄ research
	        ‚îî‚îÄ‚îÄ object_detection

Quelques commandes shell suffisent pour cr√©er les premiers niveaux de cette arborescence :

```bash	
# From within tod_tf2
(tf2) jlc@pikatchou $ mkdir -p images/faces_cubes/test
(tf2) jlc@pikatchou $ mkdir -p images/faces_cubes/train
(tf2) jlc@pikatchou $ mkdir pre_trained
(tf2) jlc@pikatchou $ mkdir -p training/faces_cubes
```
	 
## 4. T√©l√©charger le r√©seau pr√©-entra√Æn√©

Deux grandes familles de r√©seaux d√©di√©es √† la d√©tection d‚Äôobjets dans des images sont propos√©s sur Le d√©p√¥t git [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)¬†:

* Les r√©seaux __R-CNN__ (_Region-based Convolutional Neural Network_) : bas√©s sur le concept de __recherche cibl√©e__ (_selective search_). Au lieu d‚Äôappliquer une seule fen√™tre √† toutes les positions possibles de l‚Äôimage, l‚Äôalgorithme de recherche cibl√©e g√©n√®re 2000 propositions de r√©gions d‚Äôint√©r√™ts o√π il est le plus probable de trouver des objets √† d√©tecter. Cet algorithme se base sur des √©l√©ments tels que la texture, l‚Äôintensit√© et la couleur des objets qu‚Äôil a appris √† d√©tecter pour proposer des r√©gions d‚Äôint√©r√™t. Une fois les 2000 r√©gions choisies, la derni√®re partie du r√©seau produit la probabilit√© que l‚Äôobjet dans la r√©gion appartienne √† chaque classe.
Il existe √©galement des versions __Fast R-CNN__ et __Faster R-CNN__ qui permettent de rendre l‚Äôentra√Ænement plus rapide;

* Les r√©seaux __SSD__ (_Single Shot Detector_) : font partie des d√©tecteurs consid√©rant la d√©tection d‚Äôobjets comme un probl√®me de r√©gression les plus connus. L'algorithme __SSD__ utilise d‚Äôabord un r√©seau de neurones convolutif pour produire une carte des points cl√©s dans l‚Äôimage puis, comme __Faster R-CNN__, utilise des cadres de diff√©rentes tailles pour traiter les √©chelles et les ratios d‚Äôaspect.

La diff√©rence entre Faster R-CNN et SSD est qu‚Äôavec R-CNN on r√©alise une classification sur chacune des 2000 fen√™tres g√©n√©r√©es par l‚Äôalgorithme de recherche cibl√©e, alors qu‚Äôavec SSD on cherche √† pr√©dire la classe ET la fen√™tre de l‚Äôobjet, en m√™me temps. SSD apprend les d√©calages √† appliquer sur les cadres utilis√©es pour encadrer au mieux l‚Äôobjet plut√¥t que d‚Äôapprendre les fen√™tres en elle-m√™mes (ce que fait Faster R-CNN). Cela rend SSD plus rapide que Faster R-CNN, mais √©galement moins pr√©cis.

üì• Pour le travail de reconnaissance des faces des cubes dans les images fournies par la cam√©ra du robot Ergo Jr tu peux t√©l√©charger le r√©seau `SSD MobileNet V1 FPN 640x640` sur le site [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).

Une fois t√©l√©charg√©, il faut extraire l'archive TGZ au bon endroit de l'arborescence de travail :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou $ tar xvzf ~/T√©l√©chargements/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz -C pre_trained
```
puis cr√©er le dossier `ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8` dans le dossier `training/faces_cubes` :
```bash	
(tf2) jlc@pikatchou $ mkdir training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8
```
On v√©rifie :

	# From within tod_tf2
	(tf2) jlc@pikatchou $ tree -d pre_trained
	pre_trained
	‚îî‚îÄ‚îÄ ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8
	    ‚îú‚îÄ‚îÄ checkpoint
	    ‚îî‚îÄ‚îÄ saved_model
	        ‚îî‚îÄ‚îÄ variables
	        
	(tf2) jlc@pikatchou $ tree -d training
	training
	‚îî‚îÄ‚îÄ faces_cubes
	    ‚îî‚îÄ‚îÄ ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8


## 4. Cr√©ation des donn√©es pour l'apprentissage supervis√©

Ce travail se d√©compose en plusieurs √©tapes

1. Cr√©ation des images avec la cam√©ra du robot -> fichiers \*.jpg, \*.png
2. Annotation des images avec le logiciel labelImg -> fichiers \*.xml
3. Conversion des fichiers annot√©s \*.xml au format CSV
4. Conversion des fichiers annot√©s CSV au format _tensorflow record_
5. Cr√©er du fichier `label_map.pbtxt` qui contient les labels des objets √† reconna√Ætre.


### 4.1 Cr√©ation des images avec la cam√©ra du robot  

Les images des faces des cubes peuvent √™tre faites en utilisant le service ROS `/get_image` propos√© par le robot Poppy Ergo Jr.

image001.png               |  image002.png
:-------------------------:|:-------------------------:
![image1](img/image.png)   |  ![image2](img/image001.png)


ü§ñ Rappels : lancement du ROS Master et des services ROS sur le robot :
 
* allumer le robot Poppy Ergo Jr,
* se connecter sur la carte RPi du robot : `ssh pi@poppy.local` (mdp: `raspberry`) 
* ‚úÖ v√©rifier que `ROS_MASTER_URI` pointe bien vers `poppy.local:11311` :
```
pi@poppy:~ $ env|grep ROS_MASTER
ROS_MASTER_URI=http://poppy.local:11311
```	
* si `ROS_MASTER_URI` n'est pas bon, √©dite le fichier `~/.bahrc` du robot et mets la bonne valeur...
* Lance le ROS Master et les services ROS sur le robot avec la commande : `roslaunch poppy_controllers control.launch`

üíª Et maintenant dans un terminal sur ton PC :
* ‚úÖ v√©rifie que `ROS_MASTER_URI` pointe bien vers `poppy.local:11311` :
```bash
(tf2) jlc@pikatchou:~ $ env|grep ROS_MASTER
ROS_MASTER_URI=http://poppy.local:11311
```	
* si `ROS_MASTER_URI` n'est pas bon, √©dite ton fchier `~/.bahrc` entmets la bonne valeur...


üêç Tu peux maintenant utiliser le programme Python `get_image_from_ergo.py` pour cr√©er des images nomm√©es `imagesxxx.png` (`xxx` = `001`, `002`...) avec l'appui sur la touche Enter pour passer d'une prise d'image √† l'autre :
```python
import cv2, rospy
from poppy_controllers.srv import GetImage
from cv_bridge import CvBridge

i=1
while True:
    get_image = rospy.ServiceProxy("get_image", GetImage)
    response = get_image()
    bridge = CvBridge()
    image = bridge.imgmsg_to_cv2(response.image)
    cv2.imwrite(f"image{i:03d}.png", image)
    cv2.imshow("Poppy camera", image)
    rep = input("Enter...")
    #cv2.waitKey(0)
    i += 1
```

si tu obtiens l'erreur : `ModuleNotFoundError: No module named 'rospkg'`, il faut simplement ajouter le module Python `rospkg` √† ton EVP `tf2` :
```bash
(tf2) jlc@pikatchou:~ $ pip install rospkg
```


Chaque √©quipe peut faire quelques dizaines d'images variant les faces des cubes visibles puis les images peuvent √™tre d√©pos√©es sur un serveur pour servir √† toutes les √©quipes.

Une fois collect√©es toutes les images, il faut mettre environ 90 % des images dans le dossier `images\faces_cubes\train` et le reste des images dans le dossier `images\faces_cubes\test`.

### 4.2 Annoter des images avec le logiciel labelImg

L'annotation des images peut √™tre faite de fa√ßon tr√®s simple avec le logiciel `labelImg`.
C‚Äôest une √©tape du travail qui prend du temps¬†et qui peut √™tre r√©alis√©e √† plusieurs en se r√©partissant les images.

L'installation du module Python `labelImg` faite dans l'EVP `tf2` (cf section 2.) permet de lancer le logiciel `labelImg` en tapant¬†:
```bash
(tf2) jlc@pikatchou:~ $ labelImg
```

Utilise les boutons [Open Dir] et [Change Save Dir] pour te positionner dans le dossier `images/face_cubes/train/`.<br>
La premi√®re image est automatiquement charg√©e dans l'interface graphique :

![labelImg_2.png](img/labelImg_2.png)

Pour chaque image, tu dois annoter les objets √† reconna√Ætre :
* avec le bouton [Create RectBox], tu entoures une face de cube,
* la boite des label s'ouvre alors et tu dois √©crire le blabel `one` ou `two` en fonction de la face s√©lectionn√©e,
* it√®re le processus pour chacune des faces de cubes pr√©sente dans l'image...

    premi√®re face          |  deuxi√®me face            |  fin
:-------------------------:|:-------------------------:|:-------------------------:
![1](img/labelImg_3.png)   |  ![2](img/labelImg_4.png) | ![3](img/labelImg_5.png)

* quand c'est fini, tu cliques sur le bouton [Save] et tu passes √† l'image suivante avec le bouton [Next Image].
* Une fois toutes les images annot√©es, utilise les boutons [Open Dir] et [Change Save Dir] pour travailler avec les images du dossier `images/face_cubes/test/` et annote toutes les images de test.

### 4.3 Convertir les fichiers XML annot√©s au format CSV

Cette √©tape permet de synth√©tiser dans un fichier CSV unique les donn√©es d‚Äôapprentissage contenues dans les diff√©rents fichiers XML cr√©es √† l‚Äô√©tape d'annotation. 
Le programme `tod_tf2/xml_to_csv_tt.py` permet de g√©n√©rer les deux fichiers CSV correspondant aux donn√©es d‚Äôapprentissage et de test. 
Depuis le dossier `tod_tf2` tape la commande suivante¬†:

```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ python xml_to_csv_tt.py -p faces_cubes
Successfully converted xml data in file <images/faces_cubes/train_labels.csv>.
Successfully converted xml data in file <images/faces_cubes/test_labels.csv>.
```
Les fichiers `train_labels.csv` et `test_labels.csv` sont cr√©√©s dans le dossier  `images/faces_cubes/` :

### 4.4 Convertir des fichiers CSV annot√©s au format _tfrecord_

Pour cette √©tape, on utilise le programme `tod_tf2/generate_tfrecord_tt.py`. Depuis le dossier `tod_tf2` tape la commande :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ python generate_tfrecord_tt.py --project faces_cubes
Successfully created the TFRecord file: ./training/faces_cubes/train.record
Successfully created the TFRecord file: ./training/faces_cubes/test.record
```
Avec cette commande on vient de cr√©er les 2 fichiers `train.record` et `test.record` dans le dossier `training/faces_cubes`¬†: ce sont ces deux fichiers qui serviront pour l‚Äôentra√Ænement du r√©seau.

### 4.5 Cr√©er le fichier label_map.pbtxt
 
La derni√®re √©tape consiste a cr√©er le fichier `label_map.pbtxt` dans le dossier `training/faces_cubes`. Ce fichier d√©crit la ¬´¬†carte des labels¬†¬ª (label map) n√©cessaire √† l‚Äôentra√Ænement du r√©seau. 
La carte des labels permet de conna√Ætre l‚ÄôID (nombre entier) associ√© √† chaque √©tiquette (_label_) identifiant les objets √† reconna√Ætre. La structure type du fichier est la suivante¬†:


	 item {
	   id: 1
	   name: ‚Äòobjet_1‚Äô
	 }
	 item {
	   id: 2
	   name: ‚Äòobjet_2‚Äô
	 }
	 ...

Pour le projet `face_cubes`, le contenu du fichier `training/faces_cubes/label_map.pbtxt` √† cr√©er est :

	 item {
	   id: 1
	   name: ‚Äòone‚Äô
	 }
	 item {
	   id: 2
	   name: ‚Äòtwo‚Äô
	 }

## 5. Entra√Ænement supervis√© du r√©seau pr√©-entra√Æn√©

Ce travail se d√©compose en deux √©tapes

1. Cr√©er le fichier de configuration de l'entra√Ænement.
2. Lancer l'entra√Ænement supervis√©.
3. Exporter les poids du r√©seau entrain√© dans un format utilisable.



### 5.1 Cr√©ation du fichier de configuration de l'entra√Ænement.

C‚Äôest la derni√®re √©tape avant de pouvoir lancer l‚Äôentra√Ænement‚Ä¶

Le fichier de configuration `pipeline.config` pr√©sent dans le dossier du r√©seau pr√©-entra√Æn√© `pre_trained/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8` doit √™tre copi√© dans le dossier `training/faces_cubes\ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8`. 

Il faut ensuite modifier les param√®tres pour l‚Äôentra√Ænement et le test en fonction : 
* ligne 003 -- remplacer `num_classes: 90` par `num_classes: 2`, puisqu'on n'a que deux classes pour le projet `faces_cubes`
* ligne 103 -- remplacer `max_detections_per_class: 100` par `max_detections_per_class: 5:set nu
`
* ligne 104 -- remplacer `max_total_detections: 100` par `max_total_detections: 5`

* ligne 131 -- remplacer `batch_size: 64` par `batch_size: 4` : c'est le nombre d'image trait√©e avant de mettre √† jour les poids du r√©seau de neurones.<br>
Si cette valeur est trop grande, le calcul risque de d√©passer la capacit√© m√©moire RAM de ta machine... √† r√©gler en focntion de la quantit√© de RAM de ta machine.
* ligne 161 -- remplacer `fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED"` par `fine_tune_checkpoint: "pre_trained/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0", c'est le chemin du dossier pr√©-entrain√© qui contient les poids du r√©seau apr√®s le pr√©-entra√Ænement.
* ligne 162 -- remplacer `num_steps: 25000` par `num_steps: 1000`; c'est le nombre max d'it√©rations de l'entra√Ænement, pour des machines avec un CPU peu puissant on n'ira peut √™tre m√™me pas jusqu'√† 1000, ce serait trop long...
* ligne 165 -- remplacer `max_number_of_boxes: 100` par `max_number_of_boxes: 5`; on n'a pas plus de 5 faces de cubes sur une images, √ßa √©conomise de la RAM.
* ligne 167 -- remplacer `fine_tune_checkpoint_type: "classification"` par `fine_tune_checkpoint_type: "detection"` pour activer l'algorithme de d√©tection.
* ligne 172 -- remplacer `input_path: "PATH_TO_BE_CONFIGURED"` par `input_path: "./training/faces_cubes/"`, c'est le chemin du dossier contenant le fichier `train.record`.
* ligne 181 -- remplacer `label_map_path: "PATH_TO_BE_CONFIGURED"` par `label_map_path: "./training/faces_cubes/"`, c'est le chemin du dossier contenant le fichier `label_map.pbtxt`.
* ligne 185 -- remplacer `input_path: "PATH_TO_BE_CONFIGURED"` par `input_path: "./training/faces_cubes/"`, c'est le chemin du dossier contenant le fichier `test.record`.

## 5.2 Lancer l'entra√Ænement

* Copie le fichier `models\research\object_detection\model_main_tf2.py` dans la racine `tod_tf2`.
* Place toi √† la racine du projet dans le dossier `tod_tf2`.
* Tape la commande :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ python model_main_tf2.py --model_dir=training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint1  --pipeline_config_path=training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/pipeline.config 
```
Les fichiers des poids entra√Æn√©s seront √©crits dans le dossier `.../ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint1` : si tu relances l'entra√Ænement, tu peux utiliser `.../checkpoint2`, `.../checkpoint3` pour s√©par√©s des essais successifs.

Le programme Python lanc√© est tr√®s verbeux... <br>
au bout d'un temps qui peut √™tre assez long (plusieurs minutes avec un petit CPU), les logs de l'entra√Ænement apparaissent √† l'√©cran :

	...
	...
	W0507 00:24:41.010936 140206908888832 deprecation.py:531] From /home/jlc/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.
	Instructions for updating:
	Use fn_output_signature instead
	INFO:tensorflow:Step 100 per-step time 22.002s loss=0.825
	I0507 01:01:11.942076 140208909420352 model_lib_v2.py:676] Step 100 per-step time 22.002s loss=0.825
	INFO:tensorflow:Step 200 per-step time 20.926s loss=0.813
	I0507 01:36:04.090147 140208909420352 model_lib_v2.py:676] Step 200 per-step time 20.926s loss=0.813
	INFO:tensorflow:Step 300 per-step time 20.803s loss=0.801
	I0507 02:10:44.351419 140208909420352 model_lib_v2.py:676] Step 300 per-step time 20.803s loss=0.801
	INFO:tensorflow:Step 400 per-step time 20.946s loss=0.812
	I0507 02:45:38.927271 140208909420352 model_lib_v2.py:676] Step 400 per-step time 20.946s loss=0.812
	INFO:tensorflow:Step 500 per-step time 20.960s loss=0.794
	I0507 03:20:34.990385 140208909420352 model_lib_v2.py:676] Step 500 per-step time 20.960s loss=0.794
	INFO:tensorflow:Step 600 per-step time 21.045s loss=0.802
	I0507 03:55:39.516442 140208909420352 model_lib_v2.py:676] Step 600 per-step time 21.045s loss=0.802
	INFO:tensorflow:Step 700 per-step time 20.863s loss=0.786
	I0507 04:30:25.868283 140208909420352 model_lib_v2.py:676] Step 700 per-step time 20.863s loss=0.786
	INFO:tensorflow:Step 800 per-step time 20.744s loss=0.799
	I0507 05:05:00.163027 140208909420352 model_lib_v2.py:676] Step 800 per-step time 20.744s loss=0.799
	INFO:tensorflow:Step 900 per-step time 20.825s loss=0.837
	I0507 05:39:42.691898 140208909420352 model_lib_v2.py:676] Step 900 per-step time 20.825s loss=0.837
	INFO:tensorflow:Step 1000 per-step time 20.789s loss=0.778
	I0507 06:14:21.503472 140208909420352 model_lib_v2.py:676] Step 1000 per-step time 20.789s loss=0.778

Dans l'exemple ci-dessus, on voit des logs tous les 100 pas, avec environ 20 secondes par pas, soit environ 35 minutes entre chaque affichage et environ 6h de calcul pour les 1000 pas.

En cas d'arr√™t brutal du programme avec le message "Processus arr√™t√©", ne pas h√©siter √† diminer la valeur du param√®tre `batch_size` jusqu√† 2 si n√©cessaire.... <br>
M√™me avec un `batch_size` de 2, le processus Python peut n√©cessiter jusqu'√† 2 ou 3 Go de RAM pour lui tout seul, ce qui peut mettre certains portables en difficult√©...

Une fois l'entra√Ænement termin√© tu peux analyser les statistiques d'entra√Ænement avec `tensorflow` en tapant la commande :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ tensorflow --log_dir=training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint1/train
```
`tensorflow` lance un serveur HHTP en local sur ta machine, et tu peux ouvrir la page `http://` avec un navigateur pour voir les courbes d'analyse :

![tensorflow]()

### 5.3 Exporter les poids du r√©seau entra√Æn√©

On utilise le script Python `exporter_main_v2.py` du dossier `models/reasearch/object_detection/` pour extraire le __graph d'inf√©rence__ entra√Æn√© et le sauvegarder dans un fichier `saved_model.pb` qui pourra √™tre recharg√© ult√©rieurement pour exploiter le r√©seau entra√Æne√© :
```bash
# From within tod_tf2
p(tf2) jlc@pikatchou:~ $ python models/research/object_detection/exporter_main_v2.py --input_type image_tensor --pipeline_config_path training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/pipeline.config --trained_checkpoint_dir training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint1 --output_directory training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/saved1

```
Le script Python cr√©√© le fichier `saved_model.pb` dans le dossier `.../ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/saves1/saved_model` :





## 6. √âvaluation du r√©seau entra√Æn√©

On va v√©rifier que le r√©seau entra√Æn√© est bien capables de d√©tecter les faces des cubes en discriminant correctement les num√©ros √©crits sur les faces.

Le script Python `plot_object_detection_saved_model.py` permet d'exploiter le r√©seau entra√Æn√© sur des images, les arguments support√©s sont :
* `-p` : le nom du projet
* `-m` : le nom du mod√®le du r√©seau
* `-i` : le chemin du fichier image √† analyser
* `-n` : optionnel, le nombre de classes d'objets √† d√©tecter (valeur par d√©faut : )

```bash
# From within tod_tf2
p(tf2) jlc@pikatchou:~ $ python plot_object_detection_saved_model.py -p faces_cubes -m ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8 -i images/faces_cubes/test/image017.png -n 2
```


## 7. Int√©gration

Une fois le r√©seau entra√Æn√© et √©valu√©, si les r√©sultats sont bon, il ne reste plus qu'√† modifier le fichier `xxx.py` pour r√©liser les traitements suivants :

## 7.1 

1. Instancier un r√©seau en chargeant les poids du r√©seau entra√Æn√©. 
2. Utiliser le service ROS `/get_image` pour obtenir l'image faite par la cam√©ra du robot Ergo Jr,
2. D√©tecter avec le r√©seau entra√Æn√© les faces des cubes avec leur num√©ro
3. Faire afficher le r√©sulat de la d√©tection


* augmenter/diminuer `BATCH_SIZE` peut modifier les temps de calcul et la qualit√© du r√©seau entra√Æn√©...

Pour confirmer la qualit√© de votre r√©seau entra√Æn√© vous pouvez enregistrer vos propres fichiers PNG avec les images faites avec la cam√©ra du robot en utilisant le service ROS `/get_image`. 

Lancer le programme et observer les performances de votre r√©seau op√©rant sur vos propres images.

