---
title: "D√©tection d'objets avec tensorflow2"
menu:
  main:
    name: "D√©tection d'objets avec tf2"
    weight: 3
    parent: "vision"
---

Dans cette section nous allons utiliser l'API __Tensorflow Object Detection__ (_a.k.a_ TOD) qui propose :
* une collection de r√©seaux pr√©-entra√Æn√©s, sp√©cialement con√ßus pour pour la d√©tection d'objets dans des images (__Object Detection__),
* un m√©canisme de _transfert learning_ pour continuer l'entra√Ænement des r√©seaux pr√©-entra√Æn√©s avec ses propres images labellis√©es, 
pour obtenir la d√©tection des objets qui nous int√©ressent.

Contrairement √† la strat√©gie de __Classification__ pr√©sent√©e dans la section [Classification tf2](https://learn.e.ros4.pro/fr/vision/classification_tf2/), 
la __D√©tection d'objets__ permet de trouver directement les bo√Ætes englobantes des objets "face avec un 1" et "face avec un 2" : 
cette approche √©vite de faire appel au traitement d'image classique pour extraire les faces des cubes dans un premier temps, puis de classifier les images des faces des cubes dans un deuxi√®me temps. 

Le traitement d'image utilis√© pour la classification est bas√© sur une approche traditionnelle de manipulation des pixels de l'image (seuillage, extraction de contour, segmentation...).
Il est assez fragile : sensible √† la luminosit√©, √† la pr√©sence ou non d'un fond noir...

Un avantage attendu de l'approcje __Object Detection__ est de fournir directement les bo√Ætes englobantes des faces des cubes, sans passer par l'√©tape de traitement d'image.

## Pr√©requis

* BAC+2 et +
* Bonne compr√©hension de Python et numpy
* Une premi√®re exp√©rience des r√©seaux de neurones est souhaitable.

L'entra√Ænement des r√©seaux de neurones avec le module `tensorflow` se fera de pr√©f√©rence dans un environnement virtuel Python (EVP) qui permet de travailler dans un environnement Python  s√©par√© de celui existant pour le travail sous ROS.

üíª Utilise la [FAQ Python : environnement virtuel](https://learn.e.ros4.pro/fr/faq/python_venv/)  pour cr√©er un EVP :
* nomm√© `tf2`, 
* avec une version de Python √©gale √† `3.8`.
## 1. Documentation

1. Documentation g√©n√©rale sur numpy :
	* [Numpy cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)
	* [NumPy quickstart](https://numpy.org/devdocs/user/quickstart.html)

2. Documentation sur l'_API TOD_ pour `tensorflow2` :
	* Le tutoriel officiel complet : [TensorFlow 2 Object Detection API tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html)
	* Le d√©p√¥t git : [models/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)<br><br>
Le tutoriel peut √™tre consult√© pour aller chercher des d√©tails qui ne sont pas d√©velopp√©s dans l'activit√© propos√©e, mais il est pr√©ferrable de suivre 
les indications du document pr√©sent pour installer et utiliser rapidement une version r√©cente de tensorflow2. 

3. Lectures compl√©mentaires :
	* [1] [Zero to Hero: Guide to Object Detection using Deep Learning: Faster R-CNN,YOLO,SSD](https://cv-tricks.com/object-detection/faster-r-cnn-yolo-ssd/)
	* [2] [mAP (mean Average Precision) for Object Detection](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173)

## 2. Installer l'API TOD

L'installation de l'API TOD se d√©roule en 5 √©tapes :
1. Cr√©er et initialiser ton espace de travail
2. Cloner le d√©p√¥t `tensorflow/models`
3. Installer les outils `protobuf`
4. Installer l'API COCO
5. Installer le package `object_detection` 

Dans tout le document le _prompt_ du terminal sera not√© `(tf2) jlc@pikatchou $` : le pr√©fixe `(tf2)` est l√† pour bien rappeler que le travail Python pour l'API TOD se fait 
dans l'__Environnement Virtuel Python tf2__ que tu auras cr√©√© au pr√©alable (cf les Pr√©requis).


### 2.1 Cr√©er et initialiser ton espace de travail

La premi√®re √©tape consiste √† cr√©er le r√©pertoire de travail `tod_tf2`, dans lequel seront cr√©√©s tous les fichiers, et de te positionner dans ce r√©pertoire qui sera __le dossier racine du projet__.
:
```bash
(tf2) jlc@pikatchou $ cd <quelque_part>   # choisis le r√©pertoire o√π cr√©er `tod_tf2`, par exemple "cd ~/catkins_ws"
(tf2) jlc@pikatchou $ mkdir tod_tf2
(tf2) jlc@pikatchou $ cd tod_tf2/
```
Ensuite, tu clones le d√©p√¥t github `cjlux/tod_tf2_tools.git` et tu copies les fichiers `*.py` et `*.ipynb` du dossier `tod_tf2_tools` dans le dossier `tod_tf2` : 
```bash
# From tod_tf2/
(tf2) jlc@pikatchou $ git clone https://github.com/cjlux/tod_tf2_tools.git
(tf2) jlc@pikatchou $ cp tod_tf2_tools/*.py .
(tf2) jlc@pikatchou $ cp tod_tf2_tools/*.ipynb .
```

### 2.2 Cloner le d√©p√¥t `tensorflow/models`

Dans le dossier de travail `tod_tf2` clone le d√©p√¥t github `tensorflow/models`¬†(~ 635 Mo) :
```bash
# From tod_tf2/
(tf2) jlc@pikatchou $ git clone https://github.com/tensorflow/models.git
```

Tu obtiens un dossier `models`. L‚ÄôAPI TOD est dans le dossier `models/research/object_detection` :
```bash	
(tf2) jlc@pikatchou $ tree -d -L 2 .
.
‚îî‚îÄ‚îÄ models
    ‚îú‚îÄ‚îÄ community
    ‚îú‚îÄ‚îÄ official
    ‚îú‚îÄ‚îÄ orbit
    ‚îî‚îÄ‚îÄ research
```	

Compl√®te ton installation avec quelques paquets Python utiles pour le travail avec l'API TOD :

```bash
(tf2) jlc@pikatchou $ conda install cython contextlib2 pillow lxml
(tf2) jlc@pikatchou $ pip install labelimg rospkg
```
Mets √† jour la variable d‚Äôenvironnement `PYTHONPATH` en ajoutant √† la fin du fichier `~/.bashrc` les deux lignes¬†:
```bash
export TOD_ROOT="<chemin absolu du dossier tod_tf2>"
export PYTHONPATH=$TOD_ROOT/models:$TOD_ROOT/models/research:$PYTHONPATH
```
remplace `"<chemin absolu du dossier tod_tf2>"` par le chemin absolu du dossier `tod_tf2` sur ta machine.

* Lance un nouveau terminal pour activer le nouvel environnement shell¬†: tout ce qui suit sera fait dans ce nouveau terminal.
* ‚ö†Ô∏è n'oublie pas d'activer l'EVP `tf2` dans ce nouveau terminal :
```bash
jlc@pikatchou $ conda activate tf2
(tf2) jlc@pikatchou $
 ```

### 2.3 Installer les outils `protobuf`

L‚ÄôAPI native TOD utilise des fichiers `*.proto`¬†pour la configuration des mod√®les et le stockage des param√®tres d‚Äôentra√Ænement. 
Ces fichiers doivent √™tre traduits en fichiers `*.py` afin que l‚ÄôAPI Python puisse fonctionner correctement : 

* Installe d'abord le paquet debian `protobuf-compile` qui donne acc√®s √† la commande `protoc`¬†:
```bash
(tf2) jlc@pikatchou $ sudo apt install protobuf-compiler
```
* Tu peux ensuite te positionner dans le dossier `tod_tf2/models/research` et taper¬†:
```bash
# From tod_tf2/models/research/
(tf2) jlc@pikatchou $ protoc object_detection/protos/*.proto  --python_out=.
```
Cette commande travaille de fa√ßon muette.

### 2.4 Installer l'API COCO

COCO est une banque de donn√©es destin√©e √† alimenter les algorithmes de d√©tection d‚Äôobjets, de segmentation‚Ä¶ <br>
Voir [cocodataset.org](https://cocodataset.org) pour les tutoriels, publications‚Ä¶ 

Pour installer l‚ÄôAPI Python de COCO, clone le site `cocoapi.git` (~ 15 Mo) dans le dossier `/tmp`, tape la commande `make` dans le dossier `cocoapi/PythonAPI`, puis recopie le dossier `pycococtools` dans ton dossier `.../models/research/`¬†:
```bash
(tf2) jlc@pikatchou $ cd /tmp
(tf2) jlc@pikatchou $ git clone  https://github.com/cocodataset/cocoapi.git
(tf2) jlc@pikatchou $ cd cocoapi/PythonAPI/
(tf2) jlc@pikatchou $ make
(tf2) jlc@pikatchou $ cp -r pycocotools/ <chemin absolu du dossier tod_tf2>/models/research/
```
remplace `"<chemin absolu du dossier tod_tf2>"` par le chemin absolu du dossier `tod_tf2` sur ta machine (par exemple `~/catkins_ws/tod_tf2`).

### 2.5 Installer le package `object_detection` 

Pour finir l'installation, place-toi dans le dossier  `models/research/` et tape les commandes :
```bash
# From tod_tf2/models/research/
(tf2) jlc@pikatchou $ cp object_detection/packages/tf2/setup.py .
(tf2) jlc@pikatchou $ python setup.py build
(tf2) jlc@pikatchou $ pip install .
```

### 2.6 Tester l'installation de l'API TOD

Pour tester ton installation de l‚ÄôAPI TOD, place-toi dans le dossier `models/research/` et tape la commande¬†:
```bash	
# From within tod_tf2/models/research/
(tf2) jlc@pikatchou $ python object_detection/builders/model_builder_tf2_test.py
```
Le programme d√©roule toute une s√©rie de tests et doit se terminer par un OK¬†sans fiare appara√Ætre d'erreur :

	...
	[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size
	[ RUN      ] ModelBuilderTF2Test.test_session
	[  SKIPPED ] ModelBuilderTF2Test.test_session
	[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s
	I0505 18:19:38.639148 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor
	[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s
	I0505 18:19:38.640017 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture
	[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s
	I0505 18:19:38.641987 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor
	----------------------------------------------------------------------
	Ran 21 tests in 53.105s

	OK (skipped=1)

Pour finir, tu peux v√©rifier l‚Äôinstallation en utilisant le notebook IPython `object_detection_tutorial.ipynb` pr√©sent dans le dossier `tod_tf2`.<br>
(note : c'est une copie du notebook `tod_tf2/models/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb` dans laquelle on a enlev√© les cellules d'installation de l'API_TOD et quelques autres cellules qui peuvent g√©n√©rer des erreurs...).

* ‚ö†Ô∏è Avant d'ex√©cuter les cellules du notebook, il faut corriger une erreur dans le fichier `.../miniconda3/envs/tf2/lib/python3.8/site-packages/object_detection/utils/ops.py`, ligne 825 :
remplace `tf.uint8` par `tf.uint8.as_numpy_dtype`

* Dans le dossier `tod_tf2` lance la commande `jupyter notebook` et charge le notebook `object_detection_tutorial.ipynb`.
* Ex√©cute les cellules une √† une, tu ne dois pas avoir d‚Äôerreur :

	* La partie "__Detection__" (qui dure quelques secondes ou plusieurs minutes suivant ton CPU‚Ä¶) utilise le r√©seau pr√©-entra√Æn√© `ssd_mobilenet_v1_coco_2017_11_17` pour d√©tecter des objets dans les   images¬†de test :	
![notebook_test_TOD_image1et2.png](img/notebook_test_TOD_image1et2.png)

	* La partie "__Instance Segmentation__" est plus gourmande en ressources (dure de quelques dizaines de secondes √† plusieurs minutes suivant ton CPU‚Ä¶) utilise le r√©seau pr√©-entra√Æn√© `mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28` pour d√©tecter les objets et leurs masques, par exemple :
![notebook_test_TOD_image-mask1.png](img/notebook_test_TOD_image-mask1.png)

La suite du travail se d√©compose ainsi :
* Compl√©ter l'arborescence de travail
* T√©l√©charger le r√©seau pr√©-entra√Æn√©
* Cr√©er la banque d'images labellis√©es pour l'entra√Ænement supervis√© du r√©seau choisi
* Entra√Æner le r√©seau avec la banque d'images labellis√©es.
* √âvaluer les inf√©rences du r√©seau avec les images de test
* Int√©grer l'exploitation du r√©seau dans l'environnement ROS.

## 3. Compl√©ter l'arborescence de travail

L'arborescence g√©n√©rique propos√©e est la suivante :

	tod_tf2
	‚îú‚îÄ‚îÄ images
	‚îÇ   ‚îî‚îÄ‚îÄ<project>
	‚îÇ       ‚îú‚îÄ‚îÄ test
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îú‚îÄ‚îÄ train
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îî‚îÄ‚îÄ *.csv
	‚îú‚îÄ‚îÄ pre_trained
	‚îÇ	‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îú‚îÄ‚îÄ training
	‚îÇ   ‚îú‚îÄ‚îÄ<project>
	‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îÇ   ‚îú‚îÄ‚îÄ train.record
	‚îÇ   ‚îú‚îÄ‚îÄ test.record
	‚îÇ   ‚îî‚îÄ‚îÄ label_map.txt
	‚îî‚îÄ‚îÄ models
	    ‚îî‚îÄ‚îÄ research
	        ‚îî‚îÄ‚îÄ object_detection
	
* Tout ce qui est sp√©cifique au projet est plac√© dans un r√©pertoire `<project>` √† diff√©rents niveaux.

* Le dossier `images/<project>` contient pour chaque projet :
	* les dossiers `test` et `train` qui contiennent chacun :
		* les images PNG, JPG... √† analyser,
		* les fichiers d'annotation XML cr√©√©s avec le logiciel `labelImg` : ils donnent, pour chacun des objets d'une image, les coordonn√©es de la bo√Æte englobant l'objet et le label de l'objet.
	* les fichiers d'annotation CSV (contenu des fichiers XML converti au format CSV), qui seront √† leur tour convertis au format _tensorflow record_.
* Le dossier `pre_trained/` contient un sous-dossier pour chacun des r√©seaux pr√©-entrain√©s utilis√©.
* le dossier `training/<project>` contient pour chaque projet :
	* un dossier pour chaque r√©seau pr√©-entrain√© utilis√© : c'est dans ce dossier que sont stock√©s les fichiers des poids du r√©seau entra√Æn√©,
	* les fichiers `train.reccord`  et `test.reccord` : contiennent les donn√©es labelis√©es d'entra√Ænement et de test converties du format CSV au format _tensorflow record_,
	* le fichier `label_map.txt` : liste les labels correspondants aux objets √† d√©tecter.
	
Pour la d√©tection des faces des cubes dans les images de la cam√©ra du robot, le dossier `<project>` sera nomm√© `faces_cubes`, ce qui donne l'arborescence de travail :

	tod_tf2
	‚îú‚îÄ‚îÄ images
	‚îÇ   ‚îî‚îÄ‚îÄ faces_cubes
	‚îÇ       ‚îú‚îÄ‚îÄ test
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îú‚îÄ‚îÄ train
	‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png ... *.xml
	‚îÇ       ‚îî‚îÄ‚îÄ *.csv
	‚îú‚îÄ‚îÄ pre_trained
	‚îÇ	‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îú‚îÄ‚îÄ training
	‚îÇ   ‚îú‚îÄ‚îÄ faces_cubes
	‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ <pre_trained-network>
	‚îÇ   ‚îú‚îÄ‚îÄ train.record
	‚îÇ   ‚îú‚îÄ‚îÄ test.record
	‚îÇ   ‚îî‚îÄ‚îÄ label_map.txt
	‚îî‚îÄ‚îÄ models
	    ‚îî‚îÄ‚îÄ research
	        ‚îî‚îÄ‚îÄ object_detection

Quelques commandes shell suffisent pour cr√©er les premiers niveaux de cette arborescence :

```bash	
# From within tod_tf2
(tf2) jlc@pikatchou $ mkdir -p images/faces_cubes/test
(tf2) jlc@pikatchou $ mkdir -p images/faces_cubes/train
(tf2) jlc@pikatchou $ mkdir pre_trained
(tf2) jlc@pikatchou $ mkdir -p training/faces_cubes
```
V√©rifions :
```bash	
# From within tod_tf2
(tf2) jlc@pikatchou $ tree -d . -I models -I tod*
.
‚îú‚îÄ‚îÄ images
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ faces_cubes
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ test
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ train
‚îú‚îÄ‚îÄ pre_trained
‚îú‚îÄ‚îÄ tod_tf2_tools
‚îî‚îÄ‚îÄ training
    ‚îî‚îÄ‚îÄ faces_cubes
```
	 
## 4. T√©l√©charger le r√©seau pr√©-entra√Æn√©

Plusieurs familles de r√©seaux d√©di√©s √† la d√©tection d‚Äôobjets sont propos√©s sur le site  [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md), parmi lesquelles¬†:

* Les r√©seaux __R-CNN__ (_Region-based Convolutional Neural Network_) : bas√©s sur le concept de __recherche cibl√©e__ (_selective search_). 
![R-CNN](img/R-CNN.png)(source: https://arxiv.org/pdf/1311.2524.pdf)<br>
Au lieu d‚Äôappliquer la sous-fen√™tre d'analyse √† toutes les positions possibles dans l‚Äôimage, l‚Äôalgorithme de recherche cibl√©e g√©n√®re 2000 propositions de r√©gions d‚Äôint√©r√™ts o√π il est le plus probable de trouver des objets √† d√©tecter. Cet algorithme se base sur des √©l√©ments tels que la texture, l‚Äôintensit√© et la couleur des objets qu‚Äôil a appris √† d√©tecter pour proposer des r√©gions d‚Äôint√©r√™t. Une fois les 2000 r√©gions choisies, la derni√®re partie du r√©seau calcule la probabilit√© que l‚Äôobjet dans la r√©gion appartienne √† chaque classe. Les versions __Fast R-CNN__ et __Faster R-CNN__ rendent l‚Äôentra√Ænement plus efficace et plus rapide.

* Les r√©seaux __SSD__ (_Single Shot Detector_) : font partie des d√©tecteurs consid√©rant la d√©tection d‚Äôobjets comme un probl√®me de r√©gression. L'algorithme __SSD__ utilise d‚Äôabord un r√©seau de neurones convolutif pour produire une carte des points cl√©s dans l‚Äôimage puis, comme __Faster R-CNN__, utilise des cadres de diff√©rentes tailles pour traiter les √©chelles et les ratios d‚Äôaspect.

La diff√©rence entre "Faster R-CNN" et SSD est qu‚Äôavec R-CNN on r√©alise une classification sur chacune des 2000 fen√™tres g√©n√©r√©es par l‚Äôalgorithme de recherche cibl√©e, alors qu‚Äôavec SSD on cherche √† pr√©dire la classe ET la fen√™tre de l‚Äôobjet en m√™me temps. Cela rend SSD plus rapide que "Faster R-CNN", mais √©galement moins pr√©cis.

Dans le tableau du site [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md), les performances des diff√©rents r√©seaux sont exprim√©es en _COCO mAP (Mean Average Precision)_, m√©trique couramment utilis√©e pour mesurer la pr√©cision d‚Äôun mod√®le de d√©tection d‚Äôobjets. Elle consiste √† mesurer la proportion de d√©tections r√©ussies sur des images d√©j√† annot√©es du dataset COCO (Common Object in CONtext)
qui contient 200 000 images annot√©es avec 80 objets diff√©rents. Cette mesure sert de r√©f√©rence pour comparer la pr√©cision de diff√©rentes architectures de d√©tection d‚Äôobjets (plus d‚Äôinformations sur _mAP_ dans la lecture [2]).


üì• Pour le travail de d√©tection des faces des cubes dans les images fournies par la cam√©ra du robot Ergo Jr tu peux t√©l√©charger le r√©seau `Faster R-CNN ResNet50 V1 640x640` sur le site [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) (~203 Mo).

Une fois t√©l√©charg√©e, il faut extraire l'archive TGZ au bon endroit dans l'arborescence de travail :
```bash
# From within tod_tf2/
(tf2) jlc@pikatchou $ tar xvzf ~/T√©l√©chargements/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz -C pre_trained
```
puis cr√©er le dossier correspondant `faster_rcnn_resnet50_v1_640x640_coco17_tpu-8` dans le dossier `training/faces_cubes` :
```bash	
(tf2) jlc@pikatchou $ mkdir training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8
```
On v√©rifie :
```bash
# From within tod_tf2/
(tf2) jlc@pikatchou $ tree -d pre_trained
pre_trained
‚îî‚îÄ‚îÄ faster_rcnn_resnet50_v1_640x640_coco17_tpu-8
    ‚îú‚îÄ‚îÄ checkpoint
    ‚îî‚îÄ‚îÄ saved_model
        ‚îî‚îÄ‚îÄ variables
        
(tf2) jlc@pikatchou $ tree -d training
training
‚îî‚îÄ‚îÄ faces_cubes
    ‚îî‚îÄ‚îÄ faster_rcnn_resnet50_v1_640x640_coco17_tpu-8
```

## 5. Cr√©er les donn√©es pour l'apprentissage supervis√©

Cette √©tape du travail comporte cinq t√¢ches :

1. Cr√©er des images avec la cam√©ra du robot -> fichiers \*.jpg, \*.png
2. Annoter les images avec le logiciel `labelImg` -> fichiers \*.xml
3. Convertir les fichiers annot√©s XML au format CSV
4. Convertir les fichiers annot√©s CSV au format _tensorflow record_
5. Cr√©er le fichier `label_map.pbtxt` qui contient les labels des objets √† reconna√Ætre.


### 5.1 Cr√©er les images avec la cam√©ra du robot  

Les images des faces des cubes peuvent √™tre obtenues en utilisant le service ROS `/get_image` propos√© par le robot Poppy Ergo Jr.

image001.png               |  image002.png
:-------------------------:|:-------------------------:
![image1](img/image.png)   |  ![image2](img/image001.png)


ü§ñ Rappels : lancement du ROS Master et des services ROS sur le robot :
 
* allumer le robot Poppy Ergo Jr,
* se connecter sur la carte RPi du robot : `ssh pi@poppy.local` (mdp: `raspberry`) 
* ‚úÖ v√©rifier que `ROS_MASTER_URI` pointe bien vers `poppy.local:11311` :
```
pi@poppy:~ $ env|grep ROS_MASTER
ROS_MASTER_URI=http://poppy.local:11311
```	
* si `ROS_MASTER_URI` n'est pas bon, √©dite le fichier `~/.bahrc` du robot, mets la bonne valeur et tape `source ~\.bashrc`...
* Lance le ROS Master et les services ROS sur le robot avec la commande : `roslaunch poppy_controllers control.launch`

üíª Et maintenant dans un terminal sur ton PC :
* ‚úÖ v√©rifie que `ROS_MASTER_URI` pointe bien vers `poppy.local:11311` :
```bash
(tf2) jlc@pikatchou:~ $ env|grep ROS_MASTER
ROS_MASTER_URI=http://poppy.local:11311
```	
* si `ROS_MASTER_URI` n'est pas bon, √©dite le fchier `~/.bahrc`, mets la bonne valeur et tape `source ~\.bashrc`...


üêç Le programme `get_image.py` permet de visualiser l'image obtenue avec le service ROS `/getimage` du robot :

```python
import cv2, rospy
import matplotlib.pyplot as plt
import matplotlib
from poppy_controllers.srv import GetImage
from cv_bridge import CvBridge
matplotlib.use('TkAgg')

get_image = rospy.ServiceProxy("get_image", GetImage)
response  = get_image()
bridge    = CvBridge()
image     = bridge.imgmsg_to_cv2(response.image)
plt.figure()
plt.imshow(image)
plt.axis('off')
plt.show()
cv2.imwrite("image.png", image)
```

si tu obtiens l'erreur : `ModuleNotFoundError: No module named 'rospkg'`, il faut simplement ajouter le module Python `rospkg` √† ton EVP `tf2` :
```bash
(tf2) jlc@pikatchou:~ $ pip install rospkg
```

üêç Tu peux maintenant utiliser le programme Python `write_image_file.py` pour cr√©er des images des quatre cubes nomm√©es `imagesxxx.png` (`xxx` = `001`, `002`...) avec un appui sur la touche ENTER pour passer d'une prise d'image √† l'autre.


Chaque √©quipe peut faire quelques dizaines d'images en variant les faces des cubes visibles, puis les images peuvent √™tre partag√©es sur un serveur pour servir √† toutes les √©quipes.

Une fois collect√©es toutes les images, il faut mettre environ 90 % des images dans le dossier `images\faces_cubes\train` et le reste dans le dossier `images\faces_cubes\test`.

### 5.2 Annoter les images avec le logiciel labelImg

L'annotation des images peut √™tre faite de fa√ßon tr√®s simple avec le logiciel `labelImg`.
C‚Äôest une √©tape du travail qui prend du temps¬†et qui peut √™tre r√©alis√©e √† plusieurs en se r√©partissant les images √† annoter...

L'installation du module Python `labelImg` faite dans l'EVP `tf2` (cf section 2.) permet de lancer le logiciel `labelImg` en tapant¬†:
```bash
(tf2) jlc@pikatchou:~ $ labelImg
```

Utilise les boutons [Open Dir] et [Change Save Dir] pour te positionner dans le dossier `images/face_cubes/train/`.<br>
La premi√®re image est automatiquement charg√©e dans l'interface graphique :

![labelImg_2.png](img/labelImg_2.png)

Pour chaque image, tu dois annoter les objets √† reconna√Ætre :
* avec le bouton [Create RectBox], tu entoures une face d'un cube,
* la bo√Æte des labels s'ouvre alors et tu dois √©crire le blabel `one` ou `two` en fonction de la face entour√©e,
* it√®re le processus pour chacune des faces de cubes pr√©sente dans l'image...

    premi√®re face          |  deuxi√®me face            |  fin
:-------------------------:|:-------------------------:|:-------------------------:
![1](img/labelImg_3.png)   |  ![2](img/labelImg_4.png) | ![3](img/labelImg_5.png)

* quand c'est fini, tu cliques sur le bouton [Save] et tu passes √† l'image suivante avec le bouton [Next Image].
* Une fois toutes les images annot√©es, utilise les boutons [Open Dir] et [Change Save Dir] pour annoter les images de test du dossier `images/face_cubes/test/`.

### 5.3 Convertir les fichiers XML annot√©s au format CSV

Cette √©tape permet de synth√©tiser dans un fichier CSV unique les donn√©es d‚Äôapprentissage contenues dans les diff√©rents fichiers XML cr√©es √† l‚Äô√©tape d'annotation. 
Le programme `tod_tf2/xml_to_csv_tt.py` permet de g√©n√©rer les deux fichiers CSV correspondant aux donn√©es d‚Äôapprentissage et de test. 
Depuis le dossier `tod_tf2` tape la commande suivante¬†:

```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ python xml_to_csv_tt.py -p faces_cubes
Successfully converted xml data in file <images/faces_cubes/train_labels.csv>.
Successfully converted xml data in file <images/faces_cubes/test_labels.csv>.
```
Les fichiers `train_labels.csv` et `test_labels.csv` sont cr√©√©s dans le dossier  `images/faces_cubes/`.

### 5.4 Convertir les fichiers CSV annot√©s au format _tfrecord_

Pour cette √©tape, on utilise le programme `tod_tf2/generate_tfrecord_tt.py`. Depuis le dossier `tod_tf2` tape la commande :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ python generate_tfrecord_tt.py --project faces_cubes
Successfully created the TFRecord file: ./training/faces_cubes/train.record
Successfully created the TFRecord file: ./training/faces_cubes/test.record
```
Avec cette commande tu viens de cr√©er les 2 fichiers `train.record` et `test.record` dans le dossier `training/faces_cubes`¬†: ce sont les fichiers qui serviront pour l‚Äôentra√Ænement et l'√©valuation du r√©seau.

### 5.5 Cr√©er le fichier label_map.pbtxt
 
La derni√®re √©tape consiste a cr√©er le fichier `label_map.pbtxt` dans le dossier `training/faces_cubes`. 

Ce fichier d√©crit la ¬´¬†carte des labels¬†¬ª (_label map_) n√©cessaire √† l‚Äôentra√Ænement du r√©seau. 
La carte des labels permet de conna√Ætre l‚ÄôID (nombre entier) associ√© √† chaque √©tiquette (_label_) identifiant les objets √† reconna√Ætre. La structure type du fichier est la suivante¬†:


	 item {
	   id: 1
	   name: 'objet_1'
	 }
	 item {
	   id: 2
	   name: 'objet_2'
	 }
	 ...

Pour le projet `face_cubes`, le contenu du fichier `training/faces_cubes/label_map.pbtxt` √† cr√©er est :

	 item {
	   id: 1
	   name: 'one'
	 }
	 item {
	   id: 2
	   name: 'two'
	 }

## 6. Lancer l'entra√Ænement supervis√© du r√©seau pr√©-entra√Æn√©

Ce travail se d√©compose en deux √©tapes :

1. Modifier le fichier de configuration du r√©seau pr√©-entra√Æn√© pour d√©crire la configuration d'entra√Ænement.
2. Lancer l'entra√Ænement supervis√©.
3. Exporter les poids du r√©seau entrain√© dans un format utilisable.

### 6.1 Modifier le fichier de configuration

C‚Äôest la derni√®re √©tape avant de pouvoir lancer l‚Äôentra√Ænement‚Ä¶

* Le fichier de configuration `pipeline.config` pr√©sent dans le dossier `pre_trained/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8` doit √™tre copi√© dans le dossier cible `training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8`. 

* Il faut ensuite modifier les param√®tres du fichier `training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8` pour les adpater √† l'entra√Ænement :

|n¬∞ | param√®tre                     | Description                                                            | Valeur initiale  | valeur √† donner |  explication                    |
|:--|:------------------------------|:-----------------------------------------------------------------------|:----------------:|:---------------:|:--------------------------------|
|010| `num_classes`                 | nombre de classe d'objets                                              | 90               | 2               | les deux classes 'one' et 'two' |
|077| `max_detections_per_class`    | nombre max de d√©tecction par classe                                    | 100              | 4               | on met au plus 4 cubes          | 
|078| `max_total_detections`        | nombre max total de d√©tections                                         | 100              | 4               | on met au plus 4 cubes          | 
|093| `batch_size`                  | nombre d'images √† traiter en lot avant mise √† jour des poids du r√©seau | 64               | 1, 2,....       | une valeur trop √©lev√©e risque de faire d√©passer la capacit√© m√©moire RAM de ta machine... √† r√©gler en fonction de la quantit√© de RAM de ta machine.  |
|097| `num_steps`                   | Nombre max d'it√©rations d'entra√Ænement                                 | 25000             | 1000           | une valeur trop grande donne des temsp de caculs prohibitifs et un risque de sur-entra√Ænement |
|113| `fine_tune_checkpoint`        | chemin des fichiers de sauvegarde des poids du r√©seau pr√©-entra√Æn√©     | "PATH_TO_BE_<br>CONFIGURED" | "pre_trained/faster_rcnn_resnet50_v1_<br>640x640_coco17_tpu-8/checkpoint/ckpt-0" | se termine par `/ckpt-0` qui est le pr√©fixe des fichiers dans le dossier `.../checkpoint/` |
|114| `fine_tune_checkpoint_type`   | Choix de l'algorithme : "classification" ou "detection"                | "classification"| "detection"  | on veut faire de la detection d'objets |
|120| `max_number_of_boxes`         | Nombre max de bo√Ætes englobantes  dans chaque image                    | 100               | 4               | pas plus de 4 faces de cubes sur une images |
|122| `use_bfloat16`                | `true` pour les architectures TPU, `false` pour CPU                    | true              | false           | |
|126| `label_map_path`              | chemin du fichier des labels                                           | "PATH_TO_BE_<br>CONFIGURED" | "training/faces_cubes/label_map.pbtxt" | utilis√© pour l'entra√Ænement |
|128| `input_path`                  | fichier des donn√©es d'entr√©e d'entra√Ænement au format `tfrecord`       | "PATH_TO_BE_<br>CONFIGURED" | "training/faces_cubes/train.record"    | utilis√© pour l'entra√Ænement |
|139| `label_map_path`              | chemin du fichier des labels                                           | "PATH_TO_BE_<br>CONFIGURED" | "training/faces_cubes/label_map.pbtxt" | utilis√© pour l'√©valuation|
|128| `input_path`                  | fichier des donn√©es d'entr√©e de test au format `tfrecord`              | "PATH_TO_BE_<br>CONFIGURED" | "training/faces_cubes/train.record"    | utilis√© pour l'√©valuation|

## 6.2 Lancer l'entra√Ænement

* Copie le fichier `models\research\object_detection\model_main_tf2.py` dans la racine `tod_tf2`.
* Tape la commande :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou $ python model_main_tf2.py --model_dir=training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint1  --pipeline_config_path=training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/pipeline.config
```
Les fichiers des poids entra√Æn√©s seront √©crits dans le dossier `.../faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint1` : si tu relances l'entra√Ænement, tu peux utiliser `.../checkpoint2`, `.../checkpoint3` pour s√©par√©s des essais successifs.

Le programme Python lanc√© est tr√®s verbeux... <br>
au bout d'un temps qui peut √™tre assez long (plusieurs minutes avec un petit CPU), les logs de l'entra√Ænement apparaissent √† l'√©cran :

	...
	...
	W0507 00:24:41.010936 140206908888832 deprecation.py:531] From /home/jlc/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.
	Instructions for updating:
	Use fn_output_signature instead
	INFO:tensorflow:Step 100 per-step time 22.002s loss=0.825
	I0507 01:01:11.942076 140208909420352 model_lib_v2.py:676] Step 100 per-step time 22.002s loss=0.825
	INFO:tensorflow:Step 200 per-step time 20.926s loss=0.813
	I0507 01:36:04.090147 140208909420352 model_lib_v2.py:676] Step 200 per-step time 20.926s loss=0.813
	INFO:tensorflow:Step 300 per-step time 20.803s loss=0.801
	I0507 02:10:44.351419 140208909420352 model_lib_v2.py:676] Step 300 per-step time 20.803s loss=0.801
	INFO:tensorflow:Step 400 per-step time 20.946s loss=0.812
	I0507 02:45:38.927271 140208909420352 model_lib_v2.py:676] Step 400 per-step time 20.946s loss=0.812
	INFO:tensorflow:Step 500 per-step time 20.960s loss=0.794
	I0507 03:20:34.990385 140208909420352 model_lib_v2.py:676] Step 500 per-step time 20.960s loss=0.794
	INFO:tensorflow:Step 600 per-step time 21.045s loss=0.802
	I0507 03:55:39.516442 140208909420352 model_lib_v2.py:676] Step 600 per-step time 21.045s loss=0.802
	INFO:tensorflow:Step 700 per-step time 20.863s loss=0.786
	I0507 04:30:25.868283 140208909420352 model_lib_v2.py:676] Step 700 per-step time 20.863s loss=0.786
	INFO:tensorflow:Step 800 per-step time 20.744s loss=0.799
	I0507 05:05:00.163027 140208909420352 model_lib_v2.py:676] Step 800 per-step time 20.744s loss=0.799
	INFO:tensorflow:Step 900 per-step time 20.825s loss=0.837
	I0507 05:39:42.691898 140208909420352 model_lib_v2.py:676] Step 900 per-step time 20.825s loss=0.837
	INFO:tensorflow:Step 1000 per-step time 20.789s loss=0.778
	I0507 06:14:21.503472 140208909420352 model_lib_v2.py:676] Step 1000 per-step time 20.789s loss=0.778

Dans l'exemple ci-dessus, on voit des logs tous les 100 pas, avec environ 20 secondes par pas, soit environ 35 minutes entre chaque affichage et environ 6h de calcul pour les 1000 pas.

En cas d'arr√™t brutal du programme avec le message "Processus arr√™t√©", ne pas h√©siter √† diminer la valeur du param√®tre `batch_size` jusqu√† 2 si n√©cessaire.... <br>
M√™me avec un `batch_size` de 2, le processus Python peut n√©cessiter jusqu'√† 2 ou 3 Go de RAM pour lui tout seul, ce qui peut mettre certains portables en difficult√©...

Une fois l'entra√Ænement termin√© tu peux analyser les statistiques d'entra√Ænement avec `tensorflow` en tapant la commande :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ tensorflow --log_dir=training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint1/train
```
`tensorflow` lance un serveur HHTP en local sur ta machine, et tu peux ouvrir la page `http://` avec un navigateur pour voir les courbes d'analyse :

![tensorflow]()

### 6.3 Exporter les poids du r√©seau entra√Æn√©

On utilise le script Python `exporter_main_v2.py` du dossier `models/reasearch/object_detection/` pour extraire le __graph d'inf√©rence__ entra√Æn√© et le sauvegarder dans un fichier `saved_model.pb` qui pourra √™tre recharg√© ult√©rieurement pour exploiter le r√©seau entra√Æne√© :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou $ cp models\research\object_detection\exporter_main_v2.py .
(tf2) jlc@pikatchou $ python exporter_main_v2.py --input_type image_tensor --pipeline_config_path training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/pipeline.config --trained_checkpoint_dir training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/checkpoint1 --output_directory ./training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saved_model1
```
Le script Python cr√©√© le fichier `saved_model.pb` dans le dossier `.../faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saves1/saved_model` :


## 7. √âvaluation du r√©seau entra√Æn√©

On va v√©rifier que le r√©seau entra√Æn√© est bien capables de d√©tecter les faces des cubes en discriminant correctement les num√©ros √©crits sur les faces.

Le script Python `plot_object_detection_saved_model.py` permet d'exploiter le r√©seau entra√Æn√© sur des images, les arguments support√©s sont :
* `-p` : le nom du projet
* `-m` : le nom du mod√®le du r√©seau
* `-i` : le chemin du fichier image √† analyser
* `-n` : optionnel, le nombre de classes d'objets √† d√©tecter (valeur par d√©faut : )

```bash
# From within tod_tf2
(tf2) jlc@pikatchou $ python plot_object_detection_saved_model.py -p faces_cubes -s training/faces_cubes/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8/saved_model1/saved_model/ -i images/faces_cubes/test/ -n 2
```


## 8. Int√©gration

Une fois le r√©seau entra√Æn√© et √©valu√©, si les r√©sultats sont bon, il ne reste plus qu'√† modifier le fichier `xxx.py` pour r√©liser les traitements suivants :

## 8.1 

1. Instancier un r√©seau en chargeant les poids du r√©seau entra√Æn√©. 
2. Utiliser le service ROS `/get_image` pour obtenir l'image faite par la cam√©ra du robot Ergo Jr,
2. D√©tecter avec le r√©seau entra√Æn√© les faces des cubes avec leur num√©ro
3. Faire afficher le r√©sulat de la d√©tection

Lancele programme et observe les performances de ton r√©seau op√©rant sur tes propres images.

