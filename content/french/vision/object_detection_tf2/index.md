---
title: "DÃ©tection d'objets avec tensorflow2"
menu:
  main:
    name: "DÃ©tection d'objets avec tf2"
    weight: 3
    parent: "vision"
---

Dans cette section nous proposons d'utiliser l'API __Tensorflow Object Detection__ (_a.k.a_ TOD) qui propose :
* une collection de rÃ©seaux dÃ©jÃ  entraÃ®nÃ©s spÃ©cialement conÃ§us pour pour la dÃ©tection d'objets dans des images (__Object Detection__),
* le mÃ©canisme de _transfert learning_ pour continuer l'entraÃ®nement des rÃ©seaux prÃ©-entraÃ®nÃ©s avec nos propres images labellisÃ©es, 
pour obtenir la dÃ©tection des objets qui nous intÃ©ressent.

Contrairement Ã  la stratÃ©gie de __Classification__ prÃ©sentÃ©e dans dans la section [Classification tf2](https://learn.e.ros4.pro/fr/vision/classification_tf2/), 
la __DÃ©tection d'objets__ permet de trouver directement les boÃ®tes englobantes des objets "face de cube avec un 1" et "face de cube avec un 2".

Cette approche Ã©vite la phase de traitement d'image classique pour extraire puis classifier les images des faces des cubes. 

Le traitement d'image basÃ© sur une approche traditionnelle de manipulation des pixels de l'image (seuillage, extraction de contour, segmentation...) reste assez fragile : en particulier il est  sensible Ã  la luminositÃ©, Ã  la prÃ©sence ou non d'un fond noir... Un avantage attendu de l'approcje Object Detection est de fournir directement les boÃ®tes englobantes des faces des cubes sans passer par une Ã©tapde de traitement d'image.

## PrÃ©requis

* Bonne comprÃ©hension de Python et numpy
* Une premiÃ¨re expÃ©rience des rÃ©seaux de neurones est souhaitable.

L'entraÃ®nement des rÃ©seaux de neurones avec le module `tensorflow` se fera de prÃ©fÃ©rence dans un environnement virtuel Python (EVP) qui permet de travailler dans un environnement Python dÃ©diÃ©.
Vous pouvez vous rÃ©ferrer Ã  la [FAQ Python : environnement virtuel](https://learn.e.ros4.pro/fr/faq/python_venv/) 

## 1. Documentation

Documentation gÃ©nÃ©rale sur numpy :
* [Numpy cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)
* [NumPy quickstart](https://numpy.org/devdocs/user/quickstart.html)

Documentation sur l'_API TOD_ pour `tensorflow2` :
* Le tutoriel officiel complet : [TensorFlow 2 Object Detection API tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html)
* Le dÃ©pÃ´t git : [models/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)

Ce tutoriel peut Ãªtre consultÃ© pour aller chercher des dÃ©tails qui ne sont pas dÃ©veloppÃ©s dans l'activitÃ© proposÃ©e, mais il est prÃ©ferrable de suivre 
les indications du paragraphe suivant pour installer une version rÃ©cente de tensorflow2. 

## 2. Installation de l'API TOD

### 2.1 Clonage du dÃ©pÃ´t `tensorflow/models`

CrÃ©Ã© un dossier spÃ©cifique pour le travail avec l'API TOD en clonant le dÃ©pÃ´t github `cjlux/tod_tf2.git` :

```bash
(tf2) jlc@pikatchou~$ cd <quelque_part>   # choisis le rÃ©pertoire oÃ¹ cloner `tod_tf2.git`, par exemple : ~/catkin_ws/
(tf2) jlc@pikatchou~$ git clone https://github.com/cjlux/tod_tf2.git
```
Le clonage crÃ©Ã© le dossier `tod_tf2` contenant des scripts Python qui seront utilisÃ©s plus tard. Ce dossier est la racine du projet.<br>
Dans le dossier `tod_tf2` clone le dÃ©pÃ´t github `tensorflow/models`Â :
```bash
(tf2) jlc@pikatchou~$ cd tod_tf2
(tf2) jlc@pikatchou~$ git clone https://github.com/tensorflow/models.git
```

Tu obtiens un dossier `models` : lâ€™API TOD est dans le dossier `models/research/object_detection` :
```bash	
(tf2) jlc@pikatchou~$ tree -d -L 2 .
.
â””â”€â”€ models
    â”œâ”€â”€ community
    â”œâ”€â”€ official
    â”œâ”€â”€ orbit
    â””â”€â”€ research
```	

ComplÃ¨te ton installation avec quelques paquets Python utiles pour le travail avec l'API TOD :
```bash
(tf2) jlc@pikatchou $ conda install cython contextlib2 pillow lxml
(tf2) jlc@pikatchou $ pip install labelimg rospkg
```
Mets Ã  jour la variable dâ€™environnement `PYTHONPATH` en ajoutant Ã  la fin du fichier ~/.bashrc les deux lignesÂ :
```bash
export TOD_TF2="<chemin absolu du dossier tod_tf2>"
export PYTHONPATH=$TOD_TF2/models:$TOD_TF2/models/research:$PYTHONPATH
```
remplace `"<chemin absolu du dossier tod_tf2>"` par le chemin absolu du dossier `tod_tf2` sur ta machine.

Lance un nouveau terminal pour activer le nouvel environnement shellÂ ; tout ce qui suit sera fait dans ce nouveau terminal.

### 2.2 Installer les outils `protobuf`

Lâ€™API native TOD utilise des fichiers `*.proto`Â pour la configuration des modÃ¨les et le stockage des paramÃ¨tres dâ€™entraÃ®nement. 
Ces fichiers doivent Ãªtre traduits en fichiers `*.py` afin que lâ€™API Python puisse fonctionner correctement.  

Du dois installer en premier la commande `protoc`Â :
```bash
(tf2) jlc@pikatchou $ sudo apt install protobuf-compiler
```
Tu peux ensuite te positionner dans le dossier `tod_tf2/models/research` et taperÂ :
```bash
# From tod_tf2/models/research/
(tf2) jlc@pikatchou $ protoc object_detection/protos/*.proto  --python_out=.
```
Cette commande travaille de faÃ§on muette.

### 2.3 Installer l'API COCO

COCO est une banque de donnÃ©es destinÃ©e Ã  alimenter les algorithmes de dÃ©tection dâ€™objets, de segmentationâ€¦ voir [cocodataset.org](https://cocodataset.org) pour les tutoriels, publicationsâ€¦ 

ğŸ’» Pour installer lâ€™API Python de COCO, clone le site cocoapi dans le dossie `/tmp`, tape la commande `make` dans le dossier `cocoapi/PythonAPI`, puis recopie le dossier `pycococtools` dans `models/research/`Â :
```bash
(tf2) jlc@pikatchou~$ cd /tmp
(tf2) jlc@pikatchou~$ git clone  https://github.com/cocodataset/cocoapi.git
(tf2) jlc@pikatchou~$ cd cocoapi/PythonAPI/
(tf2) jlc@pikatchou~$ make
(tf2) jlc@pikatchou~$ cp -r pycocotools/ <chemin absolu du dossier tod_tf2>/models/research/
```
### 2.4 Finalisation 

ğŸ’» Pour finir l'installation, place-toi dans le dossier  `models/research/` et tape les commandes :
```bash
# From tod_tf2/models/research/
(tf2) jlc@pikatchou $ cp object_detection/packages/tf2/setup.py .
(tf2) jlc@pikatchou $ python setup.py build
(tf2) jlc@pikatchou $ pip install .
```

### 2.5 Tester l'installation de l'API TOD

ğŸ’» Pour tester ton installation de lâ€™API TOD, place-toi dans le dossier `models/research/` et tape la commandeÂ :
```bash	
# From within tod_tf2/models/research/
(tf2) jlc@pikatchou~$ python object_detection/builders/model_builder_tf2_test.py
```
Le programme dÃ©roule toute une sÃ©rie de tests et doit se terminer par un OKÂ :

	...
	[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size
	[ RUN      ] ModelBuilderTF2Test.test_session
	[  SKIPPED ] ModelBuilderTF2Test.test_session
	[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s
	I0505 18:19:38.639148 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor
	[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s
	I0505 18:19:38.640017 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture
	[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor
	INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s
	I0505 18:19:38.641987 140634691176256 test_util.py:2075] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s
	[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor
	----------------------------------------------------------------------
	Ran 21 tests in 53.105s

	OK (skipped=1)

ğŸ’» Pour finir, tu peux vÃ©rifier lâ€™installation en utilisant un cahier IPython de l'API TOD Â : dans le dossier `models/research/object_detection` lance la commande jupyter notebook et charge le cahier `colab/object_detection_tutorial.ipynb` : 

* âš ï¸ Avant d'exÃ©cuter les cellules du notebook, il faut corriger une erreur dans le fichier `.../miniconda3/envs/tf2/lib/python3.8/site-packages/object_detection/utils/ops.py`, ligne 825 :
remplacer `tf.uint8` par `tf.uint8.as_numpy_dtype`
* âš ï¸ Attention Ã  ne pas excÃ©cuter les cellules du notebook comportant les commandes `!pip install ...` ou qui contiennent la _magic chain_ `%%bash`Â :	

![notebook_test_TOD.png](img/notebook_test_TOD.png)

ExÃ©cute les cellules une Ã  une, tu ne dois pas avoir dâ€™erreur :

* La partie "__Detection__" (qui dure quelques secondes ou quelques minutes suivant ton CPUâ€¦) utilise le rÃ©seau prÃ©-entraÃ®nÃ© `ssd_mobilenet_v1_coco_2017_11_17` pour dÃ©tecter des objets dans les   imagesÂ de test :	

![notebook_test_TOD_image1et2.png](img/notebook_test_TOD_image1et2.png)

* La partie "__Instance Segmentation__" utilise le rÃ©seau prÃ©-entraÃ®nÃ© `mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28` pour dÃ©tecter les objets et leurs masques, par exemple :

![notebook_test_TOD_image-mask1.png](img/notebook_test_TOD_image-mask1.png)

La suite du travail se dÃ©compose ainsi :
* CrÃ©er l'arborescence de travail
* TÃ©lÃ©charger le rÃ©seau prÃ©-entraÃ®nÃ©
* CrÃ©er la banque d'images labellisÃ©es pour l'entraÃ®nement supervisÃ© du rÃ©seau choisi
* EntraÃ®ner le rÃ©seau avec la banque d'images labellisÃ©es.
* Ã‰valuer les infÃ©rences du rÃ©seau avec les images de test
* IntÃ©grer de l'exploitation du rÃ©seau dans l'environnement ROS.

## 3. CrÃ©er l'arborescence de travail

L'arborescence gÃ©nÃ©rique de travail proposÃ©e pour cette activitÃ© est la suivante :

	tod_tf2
	â”œâ”€â”€ images
	â”‚   â””â”€â”€<project>
	â”‚       â”œâ”€â”€ test
	â”‚       â”‚   â””â”€â”€ *.jpg, *.png ... *.xml
	â”‚       â”œâ”€â”€ train
	â”‚       â”‚   â””â”€â”€ *.jpg, *.png ... *.xml
	â”‚       â””â”€â”€ *.csv
	â”œâ”€â”€ pre_trained
	â”‚	â””â”€â”€ <pre_trained-network>
	â”œâ”€â”€ training
	â”‚   â”œâ”€â”€<project>
	â”‚   â”‚   â””â”€â”€ <pre_trained-network>
	â”‚   â”œâ”€â”€ train.record
	â”‚   â”œâ”€â”€ test.record
	â”‚   â””â”€â”€ label_map.txt
	â””â”€â”€ models
	    â””â”€â”€ research
	        â””â”€â”€ object_detection
	
	

* Le dossier `images/` contient un dossier pour chaque projet, avec dedans :
	* les dossiers `test` et `train` qui contiennent chacun :
		* les images (\*.png, \*.jpg) Ã  analyser,
		* les fichiers d'annotation (\*.xml) faits avec le logiciel `labelImg` : ils donnent, pour chacun des objets d'une image, les coordonnÃ©es de la boÃ®te englobante et le label de l'objet.
	* les fichiers d'annotation \*.cvs (convertis au format CSV), qui seront Ã  leur tour convertis au format _tensorflow record_.
* Le dossier `pre_trained/` contient un sous-dossier pour chacun des rÃ©seaux prÃ©-entrainÃ©s utilisÃ©.
* le dossier `training/` contient  Ã©galement un dossier pour chaque projet, avec dedans :
	* un dossier pour chacun des rÃ©seaux prÃ©-entrainÃ©s utilisÃ© : c'est dans ce dossier que seront stockÃ©s les fichiers des poids du rÃ©seau entraÃ®nÃ©,
	* les fichiers `train.reccord`  et `test.reccord` : contiennent les donnÃ©es labelisÃ©es d'entraÃ®nement et de test converties du format CSV au format _tensorflow record_,
	* le fichier `label_map.txt` : liste les labels correpondants aux objets Ã  dÃ©tecter.
	
Pour la dÃ©tection des faces des cubes dans les images faites par le robot Poppy Ergo Jr, le dossier `<project>` sera nommÃ© `faces_cubes`, ce qui donne l'arborescence de travail :

	tod_tf2
	â”œâ”€â”€ images
	â”‚   â””â”€â”€ faces_cubes
	â”‚       â”œâ”€â”€ test
	â”‚       â”‚   â””â”€â”€ *.jpg, *.png ... *.xml
	â”‚       â”œâ”€â”€ train
	â”‚       â”‚   â””â”€â”€ *.jpg, *.png ... *.xml
	â”‚       â””â”€â”€ *.csv
	â”œâ”€â”€ pre_trained
	â”‚	â””â”€â”€ <pre_trained-network>
	â”œâ”€â”€ training
	â”‚   â”œâ”€â”€ faces_cubes
	â”‚   â”‚   â””â”€â”€ <pre_trained-network>
	â”‚   â”œâ”€â”€ train.record
	â”‚   â”œâ”€â”€ test.record
	â”‚   â””â”€â”€ label_map.txt
	â””â”€â”€ models
	    â””â”€â”€ research
	        â””â”€â”€ object_detection

Quelques commandes shell suffisent pour crÃ©er les premiers niveaux de cette arborescence :

```bash	
# From within tod_tf2
(tf2) jlc@pikatchou $ mkdir -p images/faces_cubes/test
(tf2) jlc@pikatchou $ mkdir -p images/faces_cubes/train
(tf2) jlc@pikatchou $ mkdir pre_trained
(tf2) jlc@pikatchou $ mkdir -p training/faces_cubes
```
	 
## 4. TÃ©lÃ©charger le rÃ©seau prÃ©-entraÃ®nÃ©

Deux grandes familles de rÃ©seaux dÃ©diÃ©es Ã  la dÃ©tection dâ€™objets dans des images sont proposÃ©s sur Le dÃ©pÃ´t git [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)Â :

* Les rÃ©seaux __R-CNN__ (_Region-based Convolutional Neural Network_) : basÃ©s sur le concept de __recherche ciblÃ©e__ (_selective search_). Au lieu dâ€™appliquer une seule fenÃªtre Ã  toutes les positions possibles de lâ€™image, lâ€™algorithme de recherche ciblÃ©e gÃ©nÃ¨re 2000 propositions de rÃ©gions dâ€™intÃ©rÃªts oÃ¹ il est le plus probable de trouver des objets Ã  dÃ©tecter. Cet algorithme se base sur des Ã©lÃ©ments tels que la texture, lâ€™intensitÃ© et la couleur des objets quâ€™il a appris Ã  dÃ©tecter pour proposer des rÃ©gions dâ€™intÃ©rÃªt. Une fois les 2000 rÃ©gions choisies, la derniÃ¨re partie du rÃ©seau produit la probabilitÃ© que lâ€™objet dans la rÃ©gion appartienne Ã  chaque classe.
Il existe Ã©galement des versions __Fast R-CNN__ et __Faster R-CNN__ qui permettent de rendre lâ€™entraÃ®nement plus rapide;

* Les rÃ©seaux __SSD__ (_Single Shot Detector_) : font partie des dÃ©tecteurs considÃ©rant la dÃ©tection dâ€™objets comme un problÃ¨me de rÃ©gression les plus connus. L'algorithme __SSD__ utilise dâ€™abord un rÃ©seau de neurones convolutif pour produire une carte des points clÃ©s dans lâ€™image puis, comme __Faster R-CNN__, utilise des cadres de diffÃ©rentes tailles pour traiter les Ã©chelles et les ratios dâ€™aspect.

La diffÃ©rence entre Faster R-CNN et SSD est quâ€™avec R-CNN on rÃ©alise une classification sur chacune des 2000 fenÃªtres gÃ©nÃ©rÃ©es par lâ€™algorithme de recherche ciblÃ©e, alors quâ€™avec SSD on cherche Ã  prÃ©dire la classe ET la fenÃªtre de lâ€™objet, en mÃªme temps. SSD apprend les dÃ©calages Ã  appliquer sur les cadres utilisÃ©es pour encadrer au mieux lâ€™objet plutÃ´t que dâ€™apprendre les fenÃªtres en elle-mÃªmes (ce que fait Faster R-CNN). Cela rend SSD plus rapide que Faster R-CNN, mais Ã©galement moins prÃ©cis.

ğŸ“¥ Pour le travail de reconnaissance des faces des cubes dans les images fournies par la camÃ©ra du robot Ergo Jr tu peux tÃ©lÃ©charger le rÃ©seau `SSD MobileNet V1 FPN 640x640` sur le site [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md).

Une fois tÃ©lÃ©chargÃ©, il faut extraire l'archive TGZ au bon endroit de l'arborescence de travail :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou $ tar xvzf ~/TÃ©lÃ©chargements/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8.tar.gz -C pre_trained
```
puis crÃ©er le dossier `ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8` dans le dossier `training/faces_cubes` :
```bash	
(tf2) jlc@pikatchou $ mkdir training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8
```
On vÃ©rifie :

	# From within tod_tf2
	(tf2) jlc@pikatchou $ tree -d pre_trained
	pre_trained
	â””â”€â”€ ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8
	    â”œâ”€â”€ checkpoint
	    â””â”€â”€ saved_model
	        â””â”€â”€ variables
	        
	(tf2) jlc@pikatchou $ tree -d training
	training
	â””â”€â”€ faces_cubes
	    â””â”€â”€ ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8


## 4. CrÃ©ation des donnÃ©es pour l'apprentissage supervisÃ©

Ce travail se dÃ©compose en plusieurs Ã©tapes

1. CrÃ©ation des images avec la camÃ©ra du robot -> fichiers \*.jpg, \*.png
2. Annotation des images avec le logiciel labelImg -> fichiers \*.xml
3. Conversion des fichiers annotÃ©s \*.xml au format CSV
4. Conversion des fichiers annotÃ©s CSV au format _tensorflow record_
5. CrÃ©er du fichier `label_map.pbtxt` qui contient les labels des objets Ã  reconnaÃ®tre.


### 4.1 CrÃ©ation des images avec la camÃ©ra du robot  

Les images des faces des cubes peuvent Ãªtre faites en utilisant le service ROS `/get_image` proposÃ© par le robot Poppy Ergo Jr.

image001.png               |  image002.png
:-------------------------:|:-------------------------:
![image1](img/image.png)   |  ![image2](img/image001.png)


ğŸ¤– Rappels : lancement du ROS Master et des services ROS sur le robot :
 
* allumer le robot Poppy Ergo Jr,
* se connecter sur la carte RPi du robot : `ssh pi@poppy.local` (mdp: `raspberry`) 
* âœ… vÃ©rifier que `ROS_MASTER_URI` pointe bien vers `poppy.local:11311` :
```
pi@poppy:~ $ env|grep ROS_MASTER
ROS_MASTER_URI=http://poppy.local:11311
```	
* si `ROS_MASTER_URI` n'est pas bon, Ã©dite le fichier `~/.bahrc` du robot et mets la bonne valeur...
* Lance le ROS Master et les services ROS sur le robot avec la commande : `roslaunch poppy_controllers control.launch`

ğŸ’» Et maintenant dans un terminal sur ton PC :
* âœ… vÃ©rifie que `ROS_MASTER_URI` pointe bien vers `poppy.local:11311` :
```bash
(tf2) jlc@pikatchou:~ $ env|grep ROS_MASTER
ROS_MASTER_URI=http://poppy.local:11311
```	
* si `ROS_MASTER_URI` n'est pas bon, Ã©dite ton fchier `~/.bahrc` entmets la bonne valeur...


ğŸ Tu peux maintenant utiliser le programme Python `get_image_from_ergo.py` pour crÃ©er des images nommÃ©es `imagesxxx.png` (`xxx` = `001`, `002`...) avec l'appui sur la touche Enter pour passer d'une prise d'image Ã  l'autre :
```python
import cv2, rospy
from poppy_controllers.srv import GetImage
from cv_bridge import CvBridge

i=1
while True:
    get_image = rospy.ServiceProxy("get_image", GetImage)
    response = get_image()
    bridge = CvBridge()
    image = bridge.imgmsg_to_cv2(response.image)
    cv2.imwrite(f"image{i:03d}.png", image)
    cv2.imshow("Poppy camera", image)
    rep = input("Enter...")
    #cv2.waitKey(0)
    i += 1
```

si tu obtiens l'erreur : `ModuleNotFoundError: No module named 'rospkg'`, il faut simplement ajouter le module Python `rospkg` Ã  ton EVP `tf2` :
```bash
(tf2) jlc@pikatchou:~ $ pip install rospkg
```


Chaque Ã©quipe peut faire quelques dizaines d'images variant les faces des cubes visibles puis les images peuvent Ãªtre dÃ©posÃ©es sur un serveur pour servir Ã  toutes les Ã©quipes.

Une fois collectÃ©es toutes les images, il faut mettre environ 90 % des images dans le dossier `images\faces_cubes\train` et le reste des images dans le dossier `images\faces_cubes\test`.

### 4.2 Annoter des images avec le logiciel labelImg

L'annotation des images peut Ãªtre faite de faÃ§on trÃ¨s simple avec le logiciel `labelImg`.
Câ€™est une Ã©tape du travail qui prend du tempsÂ et qui peut Ãªtre rÃ©alisÃ©e Ã  plusieurs en se rÃ©partissant les images.

L'installation du module Python `labelImg` faite dans l'EVP `tf2` (cf section 2.) permet de lancer le logiciel `labelImg` en tapantÂ :
```bash
(tf2) jlc@pikatchou:~ $ labelImg
```

Utilise les boutons [Open Dir] et [Change Save Dir] pour te positionner dans le dossier `images/face_cubes/train/`.<br>
La premiÃ¨re image est automatiquement chargÃ©e dans l'interface graphique :

![labelImg_2.png](img/labelImg_2.png)

Pour chaque image, tu dois annoter les objets Ã  reconnaÃ®tre :
* avec le bouton [Create RectBox], tu entoures une face de cube,
* la boite des label s'ouvre alors et tu dois Ã©crire le blabel `one` ou `two` en fonction de la face sÃ©lectionnÃ©e,
* itÃ¨re le processus pour chacune des faces de cubes prÃ©sente dans l'image...

    premiÃ¨re face          |  deuxiÃ¨me face            |  fin
:-------------------------:|:-------------------------:|:-------------------------:
![1](img/labelImg_3.png)   |  ![2](img/labelImg_4.png) | ![3](img/labelImg_5.png)

* quand c'est fini, tu cliques sur le bouton [Save] et tu passes Ã  l'image suivante avec le bouton [Next Image].
* Une fois toutes les images annotÃ©es, utilise les boutons [Open Dir] et [Change Save Dir] pour travailler avec les images du dossier `images/face_cubes/test/` et annote toutes les images de test.

### 4.3 Convertir les fichiers XML annotÃ©s au format CSV

Cette Ã©tape permet de synthÃ©tiser dans un fichier CSV unique les donnÃ©es dâ€™apprentissage contenues dans les diffÃ©rents fichiers XML crÃ©es Ã  lâ€™Ã©tape d'annotation. 
Le programme `tod_tf2/xml_to_csv_tt.py` permet de gÃ©nÃ©rer les deux fichiers CSV correspondant aux donnÃ©es dâ€™apprentissage et de test. 
Depuis le dossier `tod_tf2` tape la commande suivanteÂ :

```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ python xml_to_csv_tt.py -p faces_cubes
Successfully converted xml data in file <images/faces_cubes/train_labels.csv>.
Successfully converted xml data in file <images/faces_cubes/test_labels.csv>.
```
Les fichiers `train_labels.csv` et `test_labels.csv` sont crÃ©Ã©s dans le dossier  `images/faces_cubes/` :

### 4.4 Convertir des fichiers CSV annotÃ©s au format _tfrecord_

Pour cette Ã©tape, on utilise le programme `tod_tf2/generate_tfrecord_tt.py`. Depuis le dossier `tod_tf2` tape la commande :
```bash
# From within tod_tf2
(tf2) jlc@pikatchou:~ $ python generate_tfrecord_tt.py --project faces_cubes
Successfully created the TFRecord file: ./training/faces_cubes/train.record
Successfully created the TFRecord file: ./training/faces_cubes/test.record
```
Avec cette commande on vient de crÃ©er les 2 fichiers `train.record` et `test.record` dans le dossier `training/faces_cubes`Â : ce sont ces deux fichiers qui serviront pour lâ€™entraÃ®nement du rÃ©seau.

### 4.5 CrÃ©er le fichier label_map.pbtxt
 
La derniÃ¨re Ã©tape consiste a crÃ©er le fichier `label_map.pbtxt` dans le dossier `training/faces_cubes`. Ce fichier dÃ©crit la Â«Â carte des labelsÂ Â» (label map) nÃ©cessaire Ã  lâ€™entraÃ®nement du rÃ©seau. 
La carte des labels permet de connaÃ®tre lâ€™ID (nombre entier) associÃ© Ã  chaque Ã©tiquette (_label_) identifiant les objets Ã  reconnaÃ®tre. La structure type du fichier est la suivanteÂ :


	 item {
	   id: 1
	   name: â€˜objet_1â€™
	 }
	 item {
	   id: 2
	   name: â€˜objet_2â€™
	 }
	 ...

Pour le projet `face_cubes`, le contenu du fichier `training/faces_cubes/label_map.pbtxt` Ã  crÃ©er est :

	 item {
	   id: 1
	   name: â€˜oneâ€™
	 }
	 item {
	   id: 2
	   name: â€˜twoâ€™
	 }

## 5. EntraÃ®nement supervisÃ© du rÃ©seau prÃ©-entraÃ®nÃ©

Ce travail se dÃ©compose en deux Ã©tapes

1. CrÃ©ation du fichier de configuration de l'entraÃ®nement.
2. Lancement de l'entraÃ®nement supervisÃ©.


### 5.1 CrÃ©ation du fichier de configuration de l'entraÃ®nement.

Câ€™est la derniÃ¨re Ã©tape avant de pouvoir lancer lâ€™entraÃ®nementâ€¦

Le fichier de configuration `pipeline.config` prÃ©sent dans le dossier du rÃ©seau prÃ©-entraÃ®nÃ© `pre_trained/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8` doit Ãªtre copiÃ© dans le dossier `training/faces_cubes\ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8`. 

Il faut ensuite modifier les paramÃ¨tres pour lâ€™entraÃ®nement et le test en fonction : 
* ligne 3 -- remplacer `num_classes: 90` par `num_classes: 2`, puisqu'on n'a que deux classes pour le projet `faces_cubes`
* ligne 131 -- remplacer `batch_size: 64` par `batch_size: 4` : c'est le nombre d'image traitÃ©e avant de mettre Ã  jour les poids du rÃ©seau de neurones.<br>
Si cette valeur est trop grande, le calcul risque de dÃ©passer la capacitÃ© mÃ©moire RAM de ta machine... Ã  rÃ©gler en focntion de la quantitÃ© de RAM de ta machine.
* ligne 161 -- remplacer `fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED"` par `fine_tune_checkpoint: "pre_trained/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0", c'est le chemin du dossier prÃ©-entrainÃ© qui contient les poids du rÃ©seau aprÃ¨s le prÃ©-entraÃ®nement.
* ligne 162 -- remplacer `num_steps: 25000` par `num_steps: 1000`; c'est le nombre max d'itÃ©rations de l'entraÃ®nement, pour des machines avec un CPU peu puissant on n'ira peut Ãªtre mÃªme pas jusqu'Ã  1000, ce serait trop long...
* ligne 165 -- remplacer `max_number_of_boxes: 100` par `max_number_of_boxes: 5`; on n'a pas plus de 5 faces de cubes sur une images, Ã§a Ã©conomise de la RAM.
* ligne 167 -- remplacer `fine_tune_checkpoint_type: "classification"` par `fine_tune_checkpoint_type: "detection"` pour activer l'algorithme de dÃ©tection.
* ligne 172 : remplacer `input_path: "PATH_TO_BE_CONFIGURED"` par `input_path: "./training/faces_cubes/"`, c'est le chemin du dossier contenant le fichier `train.record`.
* ligne 181 -- remplacer `label_map_path: "PATH_TO_BE_CONFIGURED"` par `label_map_path: "./training/faces_cubes/"`, c'est le chemin du dossier contenant le fichier `label_map.pbtxt`.
* ligne 185 -- remplacer `input_path: "PATH_TO_BE_CONFIGURED"` par `input_path: "./training/faces_cubes/"`, c'est le chemin du dossier contenant le fichier `test.record`.

## 5.2 Lancer l'entraÃ®nement

* copier le fichier `models\research\object_detection\model_main_tf2.py` dans la racine `tod_tf2`,
* se placer Ã  la racine du projet dans le dossier `tod_tf2`,
* taper la commande :
```bash
(tf2) jlc@pikatchou:~ $ python model_main_tf2.py --model_dir=training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/  \
                                                 --pipeline_config_path=training/faces_cubes/ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8/pipeline.config 
```
le programme Python lancÃ© est trÃ¨s verbeux... <br>
au bout d'un temps qui peut Ãªtre assez long (plusieurs minutes avec un CPU ), les logs de l'entraÃ®enement apparaissent Ã  l'Ã©cran :



En cas d'arrÃªt brutal du programme avec le message "Processus arrÃªtÃ©", ne pas hÃ©siter Ã  diminer la valeur du paramÃ¨tre `batch_size` jusquÃ  2 si nÃ©cessaire.... <br>
MÃªme avec un `batch_size` de 2, le processus Python peut nÃ©cessiter jusqu'Ã  2 ou 3 Go de RAM pour lui tout seul, ce qui peut mettre certains portables en difficultÃ©...




## 6. Ã‰valuation des infÃ©rences du rÃ©seau

Une fois le rÃ©deau entraÃ®nÃ© avec les donnÃ©es d'entraÃ®nement et de test, on peut Ã©valuer qualitativement la qualitÃ© du rÃ©seau entraÃ®nÃ© en utilisant de nouvelles images.

L'idÃ©e ici est de crÃ©er de nouvelles images et de vÃ©rifier que le rÃ©seau entraÃ®nÃ© est bien capables de dÃ©tecter les faces des cubes en discriminant correctement les numÃ©ros Ã©crits sur les faces des cubes.


## 7. IntÃ©gration

Il est maintenant temps d'intÃ©grer les deux parties du pipeline pour l'utilisation finale. Ouvrez le fichier `main.py` Ã  la racine du projet.

ExÃ©cuter maintenant le programme `main.py` : donner le chemin d'un dossier qui contient les fichiers du rÃ©seau entraÃ®nÃ© et vous devriez commencer Ã  obtenir la reconnaissance des chiffres '1' et '2' dans les images fournies.

Il faudra certainement refaire plusieurs fois l'entraÃ®nement du rÃ©seau en jouant sur plusieurs paramÃ¨tres avant d'obtenir un rÃ©seau entraÃ®nÃ© qui fonctionne correctement :

* augmenter/diminuer `BATCH_SIZE` peut modifier les temps de calcul et la qualitÃ© du rÃ©seau entraÃ®nÃ©...

Pour confirmer la qualitÃ© de votre rÃ©seau entraÃ®nÃ© vous pouvez enregistrer vos propres fichiers PNG avec les images faites avec la camÃ©ra du robot en utilisant le service ROS `/get_image`. 

Lancer le programme et observer les performances de votre rÃ©seau opÃ©rant sur vos propres images.

