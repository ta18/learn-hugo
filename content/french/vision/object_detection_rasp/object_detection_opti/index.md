---
title: "Quantification post-entra√Ænement d'un r√©seau TFLite"
menu:
  main:
    name: "Quantification post-entra√Ænement d'un r√©seau TFLite"
    weight: 3
    parent: "capsulesRSP"
---


| Classe de capsule  | &emsp;Dur√©e recommand√©e |
|:-------------------|:------------------|
| Task  &emsp;  ‚öôÔ∏è  |&emsp; 10 min      |

## üéí Pr√©requis

* BAC+2 et +
* Bonne compr√©hension de Python et numpy
* Une premi√®re exp√©rience des r√©seaux de neurones est souhaitable
* Une raspberry Pi avec cam√©ra fonctionnelle
* Capsule sur la **Mise en place des modules sur la Raspberry Pi**
* Capsule sur la **D√©tection d'objet sur la Raspberry Pi**
* Capsule sur la **Conversion d'un r√©seau Tensorflow au format TFLite**

## üéì Acquis d'apprentissage

* savoir quantifier un r√©seau Tensorflow Lite.


## üìó Documentation

Credit : 
* https://www.tensorflow.org/lite/performance/post_training_integer_quant
* https://www.tensorflow.org/lite/performance/post_training_quantization
* https://www.tensorflow.org/lite/performance/model_optimization


## Introduction

Certaines architectures embarqu√©es sont assez limit√©es 
en terme de m√©moire ou de puissance de calcul. 
Apr√®s avoir mis au format TFLite notre r√©seau entra√Æn√©, 
on souhaite maintenant l'optimiser. 
Parmi les optimisations possibles, il existe la quantification. 
D'apr√®s la documentation du Tensorflow : 


> La quantification fonctionne en r√©duisant la pr√©cision des nombres utilis√©s 
> pour repr√©senter les param√®tres d'un mod√®le, qui sont par d√©faut des nombres 
> √† virgule flottante 32 bits. Cela se traduit par une taille de mod√®le plus 
> petite et un calcul plus rapide.

Il existe plusieures mani√®res de quantifier un mod√®le selon l'utilisation 
qui va en √™tre faite. Ici, l'objectif est d'exporter ce mod√®le sur un 
microcontr√¥leur √† savoir une Raspberry Pi 4. Dans ce cas l√†, c'est la
quantification enti√®re des nombres entiers qui est adapt√©. 
Cette technique de quantification consiste √† convertir tous les poids et
sorties d'activation en donn√©es enti√®res 8 bits.

## Etalonnage des tenseurs variables

Dans un premier temps, il faut calculer les valeurs minimum et maximum des tenseurs 
√† virgule flottante (entr√©e du mod√®le, entr√©e du mod√®le, sortie du mod√®le) dans le mod√®le.
Pour ce faire, on d√©finit un ensemble de donn√©es pour les √©talonner. 

__Attention !__ L'ensemble de donn√©es doit √™tre compos√© d'au moins 100 images. 
Il faut donc faire attention √† entra√Æner son r√©seau avec plus d'une centaine d'images.

Le code ci-dessous d√©finit la fonction pour √©talonner √† partir d'un **.pb** : 

```python 
def representative_dataset():
    folder = "<path_to_train_or_test_images>"
    image_size = 640
    raw_test_data = []

    files = glob.glob(folder+'/*.png')
    for file in files:
        image = Image.open(file)
        image = image.convert("RGB")
        image = image.resize((image_size, image_size))
        #Quantizing the image between -1,1;
        image = (2.0 / 255.0) * np.float32(image) - 1.0
        image = np.asarray(image).astype(np.float32)
        image = image[np.newaxis,:,:,:]
        raw_test_data.append(image)

    for data in raw_test_data:
        yield [data]
```

√Ä des fins de test, on peut utiliser le code ci-dessous : 


```python 
def representative_dataset():
    for _ in range(100):
      data = np.random.rand(1, 640, 640, 3)
      yield [data.astype(np.float32)]
```

## Quantification

Le code suivant __convertit au format TFLite__ et quantifie un **.pb** :

```python 
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
tflite_quant_model = converter.convert()
```

Si le microcontr√¥leur est un TPU Coral Edge ou s'il ne supporte que les 
op√©rations d'entier, le mod√®le ne sera pas compatible car l'entr√©e et la sortie 
restent flottantes (float32).

Pour tester si la quantification a bien fonctionn√©, on peut afficher le type
des tenseurs d'entr√©e et de sortie  :  

```python 
interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)
input_type = interpreter.get_input_details()[0]['dtype']
print('input: ', input_type)
output_type = interpreter.get_output_details()[0]['dtype']
print('output: ', output_type)
```

Pour la premi√®re quantification o√π les entr√©es et sorties restent en flottants, 
le r√©sultat attendu est le suivant :

```python
input:  <class 'numpy.float32'>
output:  <class 'numpy.float32'>
```

## Ecriture dans un fichier

Pour exporter son r√©seau sur une architecture embarqu√©e, 
le code suivant √©crit le r√©seau dans un fichier :

```python
with open(<path_for_saving_model/model.tflite>.format(saved_model_dir), 'wb') as w:
    w.write(tflite_quant_model)
print("tflite convert complete!)
```